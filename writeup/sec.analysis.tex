\section{Analysis on Integrated Domain Data and Inference Results}

The next step after selecting a probabilistic model, is to perform analysis on the domain data and inference results. This analysis allows users to gain insight into their data and create meaningful visualizations.

Typical analysis tasks are \emph{transformations}, \emph{filtering}, \emph{ranking} and \emph{aggregation} operations. On a theoretical level, these operations can be defined in relational algebra (\cite{ozsoyouglu1987extending, klug1982equivalence}) and its extensions (\cite{rajaraman2011mining},~p.~34). Depending on how the data is stored, different languages could be used to express those operations. As SQL (\cite{iso2011sql}) is a popular language known to a broad set of people, we will present some examples in SQL. However it is straightforward to express them in any other language like sequence functions in Scala (\cite{odersky2008programming}), Pig Latin (\cite{gates2011programming}) or MapReduce (\cite{dean2008mapreduce}).

Transformations change attributes or calculate new ones without affecting the number of entities or their order. Transformations can be very diverse and range from adding a constant to a number over concatenation of strings to numerical scaling or normalization. Especially when working with probabilistic models, scaling counts to the interval (0,1) allow to interpret them as probabilities.

Filtering operations select entities that have certain properties. Relevant attributes may be either domain specific meta data, inferred variables or both. In SQL, those operations would be expressed as \texttt{WHERE} clauses. For example, a user could select documents belonging to a specific cluster $c$ (\texttt{SELECT} Title \texttt{FROM} Document \texttt{WHERE} ClusterID = $c$) or having a specific author $a$ (\texttt{SELECT} Title \texttt{FROM} Document \texttt{WHERE} Author = $a$). Combining those two filters yields documents that belong to a specific cluster and are written by a certain author, which combines both domain specific meta data and inference results.

Aggregation operations combine existing attributes to new ones, often to summarize a set of entities. Typical operations are counting, averaging, min or max operators. The concrete SQL operators depend on the SQL implementation of the database. Example aggregations are the number of documents per cluster (\texttt{SELECT} ClusterID, Count(*) \texttt{FROM} Document \texttt{GROUP BY} ClusterID) or the cluster with the maximum cluster weight $\pi$ (\texttt{SELECT} Max($\pi$) \texttt{FROM} Cluster)\footnote{If the query should also output the cluster ID, a different approach needs to be used as the maximum might not be unique. Selecting the cluster with the highest weight could then be accomplished by sorting and limiting the results to one record or using an \texttt{EXISTS} operator.}.

Ranking operations order entities by stored or calculated attributes and values. They are often combined together with aggregation operations such as counting or averaging. Rankings are used to give a summary of all entities of the same type. In SQL one would use \texttt{ORDER BY}, mostly in combination with \texttt{GROUP BY} and group aggregations. Useful rankings could be the top authors by the number of written documents (\texttt{SELECT} Author \texttt{FROM} Document \texttt{GROUP BY} Author \texttt{ORDER BY} Count(*)) or top words of a cluster that have the highest probability (\texttt{SELECT} Word.Text \texttt{FROM} Word \texttt{NATURAL JOIN} W-C \texttt{GROUP BY} W-C.ClusterID \texttt{ORDER BY} W-T.$\mu$).

Note that usually, implementations of inference algorithms for probabilistic models work with generic IDs. Depending on the application, different preprocessing steps may be necessary to assign IDs to entities. When working with customer data for example, one can use the customer ID from the customer relationship management system. On the other hand when working with image or text data, preprocessing steps are necessary to cope with these unstructured data. [TODO: move this paragraph somewhere else?]

It might come in handy if the framework offers some precalculated operations that solely depend on data of the probabilistic model. One could perform rankings or filters on the inferred data using the generic IDs or other variables of the model. This way, users that do not have a specific analysis plan in mind might use these as suggestions or to quickly explore their data without any further queries.

[TODO: \textbf{Questions}]
\begin{itemize}
\item how is the analysis related to the statistical model or similar ones?
\item what are the assumptions made when new probabilities are computed?
\item when can OLAP / POLAP be applied?
\end{itemize}
