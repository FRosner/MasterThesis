\section{Case Study: Text Topic Modeling}\label{sec:casestudy}

As a greater practical example we present a complete use case of our approach. For this purpose we use TopicExplorer, an application to explore document collections using statistical topic models (\cite{hinneburg2012topicexplorer}).

In the first subsection we explain the probabilistic model behind TopicExplorer's topic modeling and demonstrate the translation of this plate model to an ERM. This is followed by a description of the domain specific input data [TODO: What exactly? Wikipedia? Japanese blogs?]. The next subsection shows how the inference results and the meta data are integrated. In the last part we showcase some typical analysis on the results.

\subsection{Latent Dirichlet Allocation}

Latent Dirichlet Allocation (LDA) is one of the most popular topic models (\cite{blei2003latent}). Although not limited to this application, it is often used to find a hidden structure in text documents, called \emph{topics}. Such topics are, mathematically speaking, probability distributions over a vocabulary. They are often presented as a list of words having the highest probabilities. The following paragraphs will explain the probabilistic model of LDA and how it is converted to an ERM.\footnote{We will talk about smoothed LDA (\cite{blei2003latent},~p.~1006) whenever we refer to LDA. We use a slightly different notation than in the original paper to be conform with the Dirichlet multinomial clustering model used in previous sections. For demonstration purposes we assume that the prior distribution for the word proportions per topic can be topic dependent and thus the $\vec \beta_k$ is covered by the Topics plate.}

\subsubsection{Probabilistic Model}

Given a collection of documents $N$, where each document $n \in N$ consists of a set of tokens $M_n$, LDA assigns a topic $k \in K$ to each token $m \in M_n$. $\vec z_{nm}$ is a 1-of-K coded variable having a 1 at position $k$ if the token $m$ in document $n$ is assigned to topic $k$. Each topic has its own word distribution parameterized by $\vec \mu_k$. Figure~\ref{fig:topic_platemodel_original} shows LDA in plate notation.

The variables have the following constraints:
\begin{align}
\sum_{k \in K} z_{nmk} &= 1,\quad \text{a token can only be assigned to one topic}\\
\sum_{v \in V} d_{nmv} &= 1,\quad \text{a token can only be from one word type}\\
\sum_{k \in K} \theta_{nk} &= 1,\quad \text{the topic mixture of a document sums to 1}\\
\sum_{v \in V} \mu_{kv} &= 1,\quad \text{the word probabilities of a topic sum to 1}
\end{align}

The generative process is as follows:
\begin{enumerate}
\item Draw a number of tokens per document $\forall n \in N: M_n \sim \text{Poisson}(\vec \xi)$.
\item Draw word proportions per topic $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$.
\item Draw the topic mixture per document $\forall n \in N: \vec \theta_n \sim \text{Dir}(\vec \alpha)$.
\item Draw the topic assignments per token $\forall n \in N, m \in M_n: \vec z_{nm} \sim \text{Mult}(\vec \theta_n)$.
\item Draw a word type per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the topic selected by $\vec z_{nm}$.
\end{enumerate}

\subsubsection{Entity Relationship Model}

To obtain an ERM for LDA we follow the steps described in Section~\ref{sec:pm2erm}. First, transform the plate model into an APM, then translate this APM to an ERM and reduce the ERM afterwards.

\begin{figure}[t]
\begin{minipage}[t]{0.49\linewidth}
	\begin{center}
		\subfloat[LDA plate model]{\label{fig:topic_platemodel_original}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/topic_transformation_platemodel.tex}}}
	\end{center}
\end{minipage}
\hspace{0.0cm}
\begin{minipage}[t]{0.49\linewidth}
	\begin{center}
		\subfloat[Atomic LDA plate model]{\label{fig:topic_platemodel_expanded}
		\scalebox{\tikzScale}{\adjustTikzSize \input{img/topic_transformation_platemodel_expanded.tex}}}
	\end{center}
\end{minipage}\\
\caption[Transformation of LDA plate model to an APM]{Transformation of the LDA plate model to an APM. The components of the $|K|$ dimensional vector variables $\vec z_{nm}, \vec \theta_n$ and $\vec \alpha$ are then covered by the Topics plate, whereas the Words plate covers the components of the $|V|$ dimensional variables $\vec d_{nm}, \vec \mu_k$ and $\vec \beta_k$.}
\label{img:topic_platemodels}
\end{figure}

The transformation to an APM is illustrated in Figures~\ref{fig:topic_platemodel_expanded} and \ref{fig:topic_platemodel_original}. All vector variables are explicitly modeled as repeated variables for their components. The Topics plate now covers the components of the $|K|$ dimensional vector variables $\vec z_{nm}, \vec \theta_n$ and $\vec \alpha$. A new plate is created for the dimensionality $|V|$ to model the components of $\vec d_{nm}, \vec \mu_k$ and $\vec \beta_k$, representing the vocabulary of the documents.

\begin{figure}[p]
\centering
\scalebox{0.5}{\adjustTikzSize \input{img/topic_erm_good_verbose.tex}}
\caption[Verbose LDA ERM, translated from an APM]{Verbose LDA ERM, translated from an APM. The nested Token plate yields non-weak relationships from Document to D-T-W and D-T-T, respectively.}\label{fig:topic_erm_good_verbose}
\end{figure}

\begin{figure}[p]
\centering
\scalebox{0.5}{\adjustTikzSize \input{img/topic_erm_good_constraints.tex}}
\caption[LDA ERM after taking constraints into account]{LDA ERM after adjusting cardinalities and relationships according to the given variable constraints.}\label{fig:topic_erm_good_constraints}
\end{figure}

\begin{figure}[p]
\centering
\scalebox{0.5}{\adjustTikzSize \input{img/topic_erm_good_reduced.tex}}
\caption[Final LDA ERM after applying the reduction step]{Final LDA ERM after applying the reduction step.}\label{fig:topic_erm_good_reduced}
\end{figure}

In the next step, this APM is translated into a verbose ERM. Figure~\ref{fig:topic_erm_good_verbose} shows the result. The entity types Document, Word, Token and Topic correspond to the existing plates. The association entities D-T-W, D-T-T, D-T and T-W represent the plate intersections. The Token plate is nested inside the Document plate, yielding a non-weak relationship from Document to D-T-W and D-T-T, respectively.

Afterwards the constraints of the 1-of-K coded variables $\vec d_{nm}$ and $\vec z_{nm}$ are taken into account. This yields a transformation of the relationships between Word and D-T-W, and Topic and D-T-T, respectively. These relationships are no longer weak and do not contribute to the primary key of the corresponding association entity, as shown in Figure~\ref{fig:topic_erm_good_constraints}.

\subsection{Text Input Data}

\subsection{Inference Results and Meta Data Integration}

\subsection{Analysis}
