\section{Case Study: Text Topic Modeling}\label{sec:casestudy}

As a greater practical example we present a complete use case of our approach. For this purpose we use TopicExplorer, an application to explore document collections using statistical topic models (\cite{hinneburg2012topicexplorer}).

In the first subsection we explain the probabilistic model behind TopicExplorer's topic modeling and demonstrate the translation of this plate model to an ERM. This is followed by a description of the domain specific input data [TODO: What exactly? Wikipedia? Japanese blogs?]. The next subsection shows how the inference results and the meta data are integrated. In the last part we showcase some typical analysis on the results.

\subsection{Latent Dirichlet Allocation}

Latent Dirichlet Allocation (LDA) is one of the most popular topic models (\cite{blei2003latent}). Although not limited to this application, it is often used to find a hidden structure in text documents, called \emph{topics}. Such topics are, mathematically speaking, probability distributions over a vocabulary. They are often presented as a list of words having the highest probabilities. The following paragraphs will explain the probabilistic model of LDA and how it is converted to an ERM.\footnote{We will talk about smoothed LDA (\cite{blei2003latent},~p.~1006) whenever we refer to LDA. We use a slightly different notation than in the original paper to be conform with the Dirichlet multinomial clustering model used in earlier sections. For demonstration purposes we assume that the prior distribution for the word proportions per topic can be topic dependent and thus the $\vec \beta_k$ is covered by the Topics plate.}

\subsubsection{Probabilistic Model}

The generative process of LDA is as follows:
\begin{enumerate}
\item draw number of tokens per document $\forall n \in N: M_n \sim \text{Poisson}(\vec \xi)$
\item draw word proportions per topic $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$
\item draw topic mixture per document $\forall n \in N: \vec \theta_n \sim \text{Dir}(\vec \alpha)$
\item draw topic assignments per token $\forall n \in N, m \in M_n: \vec z_{nm} \sim \text{Mult}(\vec \theta_n)$
\item draw word type per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the topic selected by $\vec z_{nm}$
\end{enumerate}

\textbf{Constraints}
\begin{align}
\sum_{k \in K} z_{nmk} &= 1\\
\sum_{v \in V} d_{nmv} &= 1\\
\sum_{k \in K} \theta_{nk} &= 1\\
\sum_{v \in V} \mu_{kv} &= 1
\end{align}

\begin{figure}[t]
\centering
\scalebox{\tikzScale}{\adjustTikzSize \input{img/topic_transformation_platemodel.tex}}
\caption{Latent Dirichlet allocation plate model for text data.}\label{fig:topic_platemodel}
\end{figure}

\subsubsection{Entity Relationship Model}

\begin{figure}[t]
\begin{minipage}[t]{0.49\linewidth}
	\begin{center}
		\subfloat[Original LDA plate model]{\label{fig:topic_platemodel_original}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/topic_transformation_platemodel.tex}}}
	\end{center}
\end{minipage}
\hspace{0.0cm}
\begin{minipage}[t]{0.49\linewidth}
	\begin{center}
		\subfloat[Atomic LDA plate model]{\label{fig:topic_platemodel_expanded}
		\scalebox{\tikzScale}{\adjustTikzSize \input{img/topic_transformation_platemodel_expanded.tex}}}
	\end{center}
\end{minipage}\\
\caption{Transformation of the plate model for latent Dirichlet allocation topic models from implicit vector variables to an atomic version.}
\label{img:topic_platemodels}
\end{figure}

\subsection{Text Input Data}

\subsection{Inference Results and Meta Data Integration}

\subsection{Analysis}
