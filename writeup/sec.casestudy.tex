\section{Case Study: Text Topic Modeling}\label{sec:casestudy}

As a more complete practical example we present a self-contained use case of our approach. For this purpose we use TopicExplorer, an application to explore document collections using probabilistic topic models (\cite{hinneburg2014topic}; \cite{hinneburg2012topicexplorer}). In this case study, TopicExplorer will be used to explore and analyze Japanese blogs about the Fukushima disaster (\cite{ines2011fukushima}).

In the first subsection we explain the probabilistic model behind TopicExplorer's topic modeling and demonstrate the translation of this plate model to an ERM. This is followed by a description of the domain specific input data. The next subsection shows how the inference results and the meta data are integrated. In the last part we showcase a typical analysis of the results.

\subsection{Latent Dirichlet Allocation}

Latent Dirichlet allocation (LDA) is one of the most popular topic models (\cite{blei2003latent}). Although not limited to this application, it is often used to find a hidden structure in text documents, called \emph{topics}. Such topics are -- mathematically speaking -- probability distributions over a vocabulary. They are often presented as a list of words having the highest probabilities. The following paragraphs will explain the probabilistic model of LDA and how it is converted to an ERM.\footnote{We talk about smoothed LDA (\cite{blei2003latent},~p.~1006) whenever we refer to LDA. We use a slightly different notation than in the original paper to be conform with the Dirichlet multinomial clustering model used in previous sections. For demonstration purposes we assume that the prior distribution for the word proportions per topic can be topic dependent and thus the $\vec \beta_k$ is covered by the Topics plate.} This conversion would be done by a machine learning expert who is part of the framework developer team.

\subsubsection{Probabilistic Model}

Let $N$ be a collection of documents, where each document $n \in N$ consists of a set of tokens $M_n$. A token $m \in M_n$ is of exactly one token type which comes from a vocabulary $V$. The type of each token is 1-of-K coded in $\vec d_{nm}$ having exactly a single 1 at the index associated with $v$ if the token $m$ is of word type $v$. LDA assigns a topic $k \in K$ to each token $m \in M_n$. $\vec z_{nm}$ is a 1-of-K coded variable having a 1 at the index associated with $k$ if the token $m$ in document $n$ is assigned to topic $k$. Each topic has its own word distribution parameterized by $\vec \mu_k$. The topic proportions per document are represented as $\vec \theta_n$. To adopt a Bayesian view, hyper-parameters $\vec \alpha$ and $\vec \beta_k$ for $\vec \theta_n$ and $\vec \mu_k$ are added to the model. Figure~\ref{fig:topic_platemodel_original} shows LDA in plate notation.

The variables have the following constraints:
\begin{align}
\sum_{k \in K} z_{nmk} &= 1,\quad \text{a token is assigned to exactly one topic}\\
\sum_{v \in V} d_{nmv} &= 1,\quad \text{a token is exactly from one word type}\\
\sum_{k \in K} \theta_{nk} &= 1,\quad \text{the topic mixture of a document sums to 1}\\
\sum_{v \in V} \mu_{kv} &= 1,\quad \text{the word probabilities of a topic sum to 1}
\end{align}

\begin{figure}[t]
\begin{minipage}[t]{0.49\linewidth}
	\begin{center}
		\subfloat[LDA plate model]{\label{fig:topic_platemodel_original}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/topic_transformation_platemodel.tex}}}
	\end{center}
\end{minipage}
\hspace{0.0cm}
\begin{minipage}[t]{0.49\linewidth}
	\begin{center}
		\subfloat[Atomic LDA plate model]{\label{fig:topic_platemodel_expanded}
		\scalebox{\tikzScale}{\adjustTikzSize \input{img/topic_transformation_platemodel_expanded.tex}}}
	\end{center}
\end{minipage}\\
\caption[Transformation of LDA plate model to an APM]{Transformation of the LDA plate model to an APM. The components of the $|K|$-dimensional vector variables $\vec z_{nm}, \vec \theta_n$ and $\vec \alpha$ are then covered by the Topics plate, whereas the Words plate covers the components of the $|V|$-dimensional variables $\vec d_{nm}, \vec \mu_k$ and $\vec \beta_k$.}
\label{img:topic_platemodels}
\end{figure}

The joint distribution factorization is given as
\begin{multline}
P(\boldsymbol{d},\boldsymbol{z}, \boldsymbol{\mu}, \boldsymbol{\theta} | \vec \alpha, \boldsymbol{\beta}) =
 \left(\prod_{n \in N} P(\vec \theta_n | \vec \alpha)\right) \cdot
 \left(\prod_{n \in N} \prod_{m \in M_n} P(\vec z_{nm} | \vec \theta_n)\right) \cdot \\
 \left(\prod_{k \in K} P(\vec \mu_k | \vec \beta_k)\right) \cdot
 \left(\prod_{n \in N} \prod_{m \in M_n} \prod_{k \in K}P(\vec d_{nm} | \vec z_{nm}, \vec \mu_k)\right),
\end{multline}
where $\boldsymbol{d} = \{\vec d_{nm} : n \in N \land m \in M_n \}$ is a short notation for all token vectors, $\boldsymbol{z} = \{ \vec z_{nm} : n \in N \land m \in M_n \}$ are the cluster assignments of all tokens, $\boldsymbol{\mu} = \{ \vec \mu_k : k \in K \}$ represents the word proportions of all topics, $\boldsymbol{\theta} = \{ \vec \theta_n : n \in N \}$ stands for the topic mixtures of all documents and $\boldsymbol{\beta} = \{ \vec \beta_k : k \in K \}$ denotes the set of all hyper-parameters for the word proportions.

The generative process is as follows:
\begin{enumerate}
\item Draw word proportions per topic $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$.
\item Draw the topic mixture per document $\forall n \in N: \vec \theta_n \sim \text{Dir}(\vec \alpha)$.
\item Draw the topic assignments per token $\forall n \in N, m \in M_n: \vec z_{nm} \sim \text{Mult}(\vec \theta_n)$.
\item Draw a word type per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the topic selected by $\vec z_{nm}$.
\end{enumerate}

\subsubsection{Entity-Relationship Model}

To obtain an ERM for LDA we follow the steps described in Section~\ref{sec:pm2erm}. First, transform the plate model into an APM, then translate this APM to an ERM and reduce the ERM afterwards.

The transformation to an APM is illustrated in Figures~\ref{fig:topic_platemodel_original} and \ref{fig:topic_platemodel_expanded}. All vector variables are explicitly modeled as repeated variables for their components. The Topics plate now covers the components of the $|K|$-dimensional vector variables $\vec z_{nm}, \vec \theta_n$ and $\vec \alpha$. A new plate is created for the dimensionality $|V|$ to model the components of $\vec d_{nm}, \vec \mu_k$ and $\vec \beta_k$, representing the vocabulary of the documents.

\begin{figure}[p]
\centering
\scalebox{0.5}{\adjustTikzSize \input{img/topic_erm_good_verbose.tex}}
\caption[Verbose LDA ERM, translated from an APM]{Verbose LDA ERM, translated from an APM. The nested Token plate yields non-weak relationships from Document to D-T-W and D-T-T.}\label{fig:topic_erm_good_verbose}
\end{figure}

\begin{figure}[p]
\centering
\scalebox{0.5}{\adjustTikzSize \input{img/topic_erm_good_constraints.tex}}
\caption[LDA ERM after taking constraints into account]{LDA ERM after adjusting cardinalities and relationships according to the given variable constraints.}\label{fig:topic_erm_good_constraints}
\end{figure}

\begin{figure}[p]
\centering
\scalebox{0.5}{\adjustTikzSize \input{img/topic_erm_good_reduced.tex}}
\caption[Final LDA ERM after applying the reduction step]{Final LDA ERM after applying the reduction step.}\label{fig:topic_erm_good_reduced}
\end{figure}

In the next step, this APM is translated to a verbose ERM. Figure~\ref{fig:topic_erm_good_verbose} shows the result. The entity types Document, Word, Token and Topic correspond to the existing plates. The association entities D-T-W, D-T-T, D-T and T-W represent the plate intersections. The Token plate is nested inside the Document plate, yielding non-weak relationships from Document to D-T-W and D-T-T.

Afterwards, the constraints of the 1-of-K coded variables $\vec d_{nm}$ and $\vec z_{nm}$ are taken into account. This yields a transformation of the relationships between Word and D-T-W, and Topic and D-T-T, respectively. These relationships are no longer weak and do not contribute to the primary key of the corresponding association entity, as shown in Figure~\ref{fig:topic_erm_good_constraints}.

Finally, the still verbose looking ERM is reduced as shown in Figure~\ref{fig:topic_erm_good_reduced}. The Token entity type is merged with D-T-W and D-T-T. The resulting duplicate relationship between Document and Token is merged as well. The reduced ERM is exactly as one would expect it when designing a data model for LDA from scratch.

A document consists of one or more tokens which are of exactly one word type. Each token is assigned to a topic, while one topic can have multiple tokens assigned. The inferred topic mixture for each document is stored in D-T.$\theta$, while T-W.$\mu$ holds the word probabilities for each topic. As in our LDA model, the prior parameter for the topic mixture $\alpha$ is the same for every document: It resides as an attribute of the Topic entity type. The individual priors for the word distributions are stored in T-W.$\beta$.

\subsection{Topic Modeled Japanese Blog Entries}

\subsubsection{Text Input Data}

Input data for this case study will be from Japanese blogs about the Fukushima disaster. A crawler collects blog entries using Kizasi, a Japanese blog search engine.\footnote{\url{http://kizasi.jp} indexes and stores blog entries from different blog providers.} The search engine is queried using keywords such as ``nuclear power'' (\begin{CJK}{UTF8}{}原発\end{CJK}) and the results are filtered depending on the concrete question, e.g. using additional keywords like ``caesium'' (\begin{CJK}{UTF8}{}セシウム\end{CJK}) and ``beef'' (\begin{CJK}{UTF8}{}ビーフ\end{CJK}), to investigate public opinions regarding caesium concentrations in beef following the accident at the nuclear power plant.

Every blog entry is then preprocessed. Most of the preprocessing tasks are done by MeCab, a Japanese morphological analyzer.\footnote{\url{http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html}} A tokenizer extracts tokens from the full text, which are then used for detecting sentence boundaries. Afterwards, MeCab performs lemmatizing and part-of-speech tagging. The tokens are stored in their lemmatized form together with their part-of-speech tag (noun, verb or adjective) and their position in the text. The full text of each entry is stored in raw format as well to be able to view it in the user interface. Additionally, the blog entry title, its URL and the publication time are stored. Identical types of tokens are called a \emph{word} or \emph{word type}. Word types are extracted from all the tokens.

To allow further analysis for a better visualization of the topics, tokens are combined to \emph{frames}. A frame is defined as a noun-verb combination appearing together in a fixed-size window inside a document.\footnote{\textcite{minsky1977frame} defines frames as basic units for transmitting semantic context. More information about this concept can also be found in the book ``Natural language semantics'' (\cite{allan2001natural}).} Frames are later filtered depending on the topic that is assigned to the participating tokens.

The resulting ERM after the crawling, preprocessing, and frame extraction is shown in Figure~\ref{fig:TopicExplorer_data_erm}. The domain expert provides this ERM together with his application programmer who implements the preprocessing steps that are required to fill the ERM with data.

\begin{figure}[p]
\centering
\scalebox{\tikzScale}{\adjustTikzSize \input{img/TopicExplorer_data_erm.tex}}
\caption[TopicExplorer input data model]{TopicExplorer input data model. Japanese blog entries are tokenized, lemmatized and part-of-speech tagged and stored together with additional meta data.}\label{fig:TopicExplorer_data_erm}
\end{figure}

\begin{figure}[p]
\centering
\scalebox{0.7}{\adjustTikzSize \input{img/TopicExplorer_integrated_erm.tex}}
\caption[Integrated TopicExplorer data model]{Integrated TopicExplorer data model. The general LDA ERM template (Figure~\ref{fig:topic_erm_good_reduced}) is integrated into the text input data model (Figure~\ref{fig:TopicExplorer_data_erm}).}\label{fig:TopicExplorer_integrated_erm}
\end{figure}

\subsubsection{Analysis and Visualization}
\label{subsec:casestudy_analysis}

To analyze the topic structure of blog entries, the framework user selects the LDA ERM template and matches the entities of the domain specific data model with it. Figure~\ref{fig:TopicExplorer_integrated_erm} shows the integrated data model. A blog entry is treated as a document, matching tokens and words is trivial. The integrated model contains additional domain specific constructs such as frames, as well as the LDA specific ones like topics.

As a basic visualization it is possible to present entities by their attribute values using simple relational algebra operations such as selections, projections and joins. One could visualize the document collection by presenting a list of blog entry titles that expand to the full text when selected. Tokens can be visualized in their preprocessed form or, together with their position index, in the full text of their assigned document. Visualizing the vocabulary might happen as a list of words. However, it is not trivial to visualize a topic, as it is -- mathematically speaking -- just a probability distribution. Although displaying a probability distribution as a bar plot for example is a well-known practice, it may be hard to understand for some users. Additionally, the vocabulary is usually rather big and a simple bar plot of all probabilities is difficult to draw.

A sophisticated visualization which aims to be more easily understood, requires more complex analysis steps on the integrated data. We present a few of them that are part of the current version of TopicExplorer (\cite{hinneburg2014topic}).

\begin{description}
\item[Entry topic mixture.] LDA assigns a vector of topic probabilities stored in D-T.$\theta$ for each entry, called a topic mixture. This could either be presented as a list of topics with decreasing order of probabilities or as a pie chart like in the Topic Model Visualization Engine by \textcite{chaney2012visualizing}.

\item[Topic entries.] Reversing the idea behind the entry topic mixture, one can visualize a topic as a list of the most representative documents for this topic. This is done by joining Entry, Token and Topic, grouping by the entry and topic IDs and counting the number of tokens in each group. For each topic the entries are sorted with decreasing count, yielding a list of representative documents.

\item[Topic words.] As stated above, a topic can be represented as a list of words that have a high probability for that topic (T-W.$\mu$). This list can be cut off, yielding a top words visualization for each topic.

\item[Topic frames.] Another visualization of topics uses the concept of frames. First, frames whose noun and verb are not assigned to the same topic are filtered out. The remaining frames are called \emph{topic frames}. Instead of visualizing a topic as top words, top frames can be used. This is done by joining the topic frames view with Token, Word and Topic, grouping by the topic ID and the word IDs of the frame tokens, and counting the number of frames having equal word types per topic. This list can then be sorted and limited.

\item[Topic time.] To analyze how the discussion about Fukushima develops, one can visualize how the topics change over time. If the framework does not offer a topic model that explicitly models a dynamic component like proposed by \textcite{blei2006dynamic}, it is possible to analyze the dynamics of the topics after using simple LDA. To achieve this, entries are grouped depending on their publication date, e.g. monthly. Given those two attributes Entry.Year and Entry.Month\footnote{Depending on the implementation of the database, those attributes can be extracted on the fly out of the stored timestamp.}, the Entry table joined with Topic is grouped by the topic ID, Entry.Year and Entry.Month. Finally, counting the tokens assigned to each topic at each point of time allows plotting a time line for each topic.
\end{description}

The domain expert defines which analysis to perform and how to visualize the results. The application programmer implements the transformations.
