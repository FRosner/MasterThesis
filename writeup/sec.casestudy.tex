\section{Case Study: Text Topic Modeling}\label{sec:casestudy}

As a greater practical example we present a complete use case of our approach. For this purpose we use TopicExplorer, an application to explore document collections using probabilistic topic models (\cite{hinneburg2012topicexplorer}) [TODO: cite current paper as well]. In this case study, TopicExplorer will be used to explore and analyze Japanese blogs about the Fukushima disaster (\cite{ines2011fukushima}).

In the first subsection we explain the probabilistic model behind TopicExplorer's topic modeling and demonstrate the translation of this plate model to an ERM. This is followed by a description of the domain specific input data. The next subsection shows how the inference results and the meta data are integrated. In the last part we showcase some typical analysis on the results.

[TODO: show roles of application programmer and domain expert]

\subsection{Latent Dirichlet Allocation}

Latent Dirichlet Allocation (LDA) is one of the most popular topic models (\cite{blei2003latent}). Although not limited to this application, it is often used to find a hidden structure in text documents, called \emph{topics}. Such topics are, mathematically speaking, probability distributions over a vocabulary. They are often presented as a list of words having the highest probabilities. The following paragraphs will explain the probabilistic model of LDA and how it is converted to an ERM.\footnote{We will talk about smoothed LDA (\cite{blei2003latent},~p.~1006) whenever we refer to LDA. We use a slightly different notation than in the original paper to be conform with the Dirichlet multinomial clustering model used in previous sections. For demonstration purposes we assume that the prior distribution for the word proportions per topic can be topic dependent and thus the $\vec \beta_k$ is covered by the Topics plate.}

\subsubsection{Probabilistic Model}

Given a collection of documents $N$, where each document $n \in N$ consists of a set of tokens $M_n$, LDA assigns a topic $k \in K$ to each token $m \in M_n$. $\vec z_{nm}$ is a 1-of-K coded variable having a 1 at position $k$ if the token $m$ in document $n$ is assigned to topic $k$. Each topic has its own word distribution parameterized by $\vec \mu_k$. Figure~\ref{fig:topic_platemodel_original} shows LDA in plate notation.

The variables have the following constraints:
\begin{align}
\sum_{k \in K} z_{nmk} &= 1,\quad \text{a token can only be assigned to one topic}\\
\sum_{v \in V} d_{nmv} &= 1,\quad \text{a token can only be from one word type}\\
\sum_{k \in K} \theta_{nk} &= 1,\quad \text{the topic mixture of a document sums to 1}\\
\sum_{v \in V} \mu_{kv} &= 1,\quad \text{the word probabilities of a topic sum to 1}
\end{align}

\begin{figure}[t]
\begin{minipage}[t]{0.49\linewidth}
	\begin{center}
		\subfloat[LDA plate model]{\label{fig:topic_platemodel_original}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/topic_transformation_platemodel.tex}}}
	\end{center}
\end{minipage}
\hspace{0.0cm}
\begin{minipage}[t]{0.49\linewidth}
	\begin{center}
		\subfloat[Atomic LDA plate model]{\label{fig:topic_platemodel_expanded}
		\scalebox{\tikzScale}{\adjustTikzSize \input{img/topic_transformation_platemodel_expanded.tex}}}
	\end{center}
\end{minipage}\\
\caption[Transformation of LDA plate model to an APM]{Transformation of the LDA plate model to an APM. The components of the $|K|$ dimensional vector variables $\vec z_{nm}, \vec \theta_n$ and $\vec \alpha$ are then covered by the Topics plate, whereas the Words plate covers the components of the $|V|$ dimensional variables $\vec d_{nm}, \vec \mu_k$ and $\vec \beta_k$.}
\label{img:topic_platemodels}
\end{figure}

The generative process is as follows:
\begin{enumerate}
\item Draw a number of tokens per document $\forall n \in N: M_n \sim \text{Poisson}(\vec \xi)$.
\item Draw word proportions per topic $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$.
\item Draw the topic mixture per document $\forall n \in N: \vec \theta_n \sim \text{Dir}(\vec \alpha)$.
\item Draw the topic assignments per token $\forall n \in N, m \in M_n: \vec z_{nm} \sim \text{Mult}(\vec \theta_n)$.
\item Draw a word type per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the topic selected by $\vec z_{nm}$.
\end{enumerate}

\subsubsection{Entity Relationship Model}

To obtain an ERM for LDA we follow the steps described in Section~\ref{sec:pm2erm}. First, transform the plate model into an APM, then translate this APM to an ERM and reduce the ERM afterwards.

The transformation to an APM is illustrated in Figures~\ref{fig:topic_platemodel_expanded} and \ref{fig:topic_platemodel_original}. All vector variables are explicitly modeled as repeated variables for their components. The Topics plate now covers the components of the $|K|$ dimensional vector variables $\vec z_{nm}, \vec \theta_n$ and $\vec \alpha$. A new plate is created for the dimensionality $|V|$ to model the components of $\vec d_{nm}, \vec \mu_k$ and $\vec \beta_k$, representing the vocabulary of the documents.

\begin{figure}[p]
\centering
\scalebox{0.5}{\adjustTikzSize \input{img/topic_erm_good_verbose.tex}}
\caption[Verbose LDA ERM, translated from an APM]{Verbose LDA ERM, translated from an APM. The nested Token plate yields non-weak relationships from Document to D-T-W and D-T-T, respectively.}\label{fig:topic_erm_good_verbose}
\end{figure}

\begin{figure}[p]
\centering
\scalebox{0.5}{\adjustTikzSize \input{img/topic_erm_good_constraints.tex}}
\caption[LDA ERM after taking constraints into account]{LDA ERM after adjusting cardinalities and relationships according to the given variable constraints.}\label{fig:topic_erm_good_constraints}
\end{figure}

\begin{figure}[p]
\centering
\scalebox{0.5}{\adjustTikzSize \input{img/topic_erm_good_reduced.tex}}
\caption[Final LDA ERM after applying the reduction step]{Final LDA ERM after applying the reduction step.}\label{fig:topic_erm_good_reduced}
\end{figure}

In the next step, this APM is translated into a verbose ERM. Figure~\ref{fig:topic_erm_good_verbose} shows the result. The entity types Document, Word, Token and Topic correspond to the existing plates. The association entities D-T-W, D-T-T, D-T and T-W represent the plate intersections. The Token plate is nested inside the Document plate, yielding a non-weak relationship from Document to D-T-W and D-T-T, respectively.

Afterwards, the constraints of the 1-of-K coded variables $\vec d_{nm}$ and $\vec z_{nm}$ are taken into account. This yields a transformation of the relationships between Word and D-T-W, and Topic and D-T-T, respectively. These relationships are no longer weak and do not contribute to the primary key of the corresponding association entity, as shown in Figure~\ref{fig:topic_erm_good_constraints}.

Finally, the still verbose looking ERM is reduced as shown in Figure~\ref{fig:topic_erm_good_reduced}. The Token entity type is merged with D-T-W and D-T-T. The resulting duplicate relationship between Document and Token is merged as well. The reduced ERM is exactly what one would expect when designing a data model for LDA from scratch.

A document consists of one or more tokens which are of exactly one word type. Each token is assigned to a topic, while one topic can have multiple tokens assigned. The inferred topic mixture for each document is stored in D-T.$\theta$, while T-W.$\mu$ holds the word probabilities for each topic. As in our LDA model the prior for the topic mixture is the same for every document it resides as an attribute of the Topic entity type. The individual priors for the word distributions are stored in T-W.$\beta$.

\subsection{Topic Modeled Japanese Blog Entries}

\subsubsection{Text Input Data}

Input data for this example will be from Japanese blogs about the Fukushima disaster. A crawler collects blog entries using Kizasi, a Japanese blog search engine.\footnote{\url{http://kizasi.jp}} Kizasi indexes and stores blog entries from different blog providers. The search engine is queried using keywords such as ``nuclear power'' or ``nuclear power plant'' (\begin{CJK}{UTF8}{}原発\end{CJK}) and the results are filtered depending on the concrete question, e.g. using additional keywords like ``caesium'' (\begin{CJK}{UTF8}{}セシウム\end{CJK}) and ``beef'' (\begin{CJK}{UTF8}{}ビーフ\end{CJK}).

Every blog entry is then preprocessed. Most of the preprocessing tasks are done by MeCab, a Japanese morphological analyzer.\footnote{\url{http://sourceforge.net/projects/mecab/} or \url{http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html}} A tokenizer extracts tokens from the full text, which are then used for detecting sentence boundaries. MeCab then performs lemmatizing and part-of-speech tagging. The tokens are stored in their lemmatized form together with their part-of-speech tag (noun, verb or adjective) and their position in the text. The full text of each entry is stored in raw format as well to be able to view it to the user. Additionally, the blog entry title, its URL and the publication time are stored. Same types of tokens are called a \emph{word} or \emph{word type}. Word types are extracted from all the tokens and stored with UTF-8 characters as their name. [TODO: Ask Alex if the previous paragraphs are correct]

To allow further analysis for a better visualization of the topics, tokens are combined to \emph{frames}. A frame in this case is defined as a noun verb combination appearing together in a window of a predefined size inside a document.\footnote{\textcite{minsky1977frame} defined frames as basic units for transmitting semantic context. More information about this concept can also be found in the book ``Natural language semantics'' by \textcite{allan2001natural}.} Frames are later filtered depending on the topic that is assigned to the participating tokens. This is explained in Section~\ref{subsec:casestudy_analysis}

The resulting ERM after the crawling, preprocessing and frame extraction is shown in Figure~\ref{fig:TopicExplorer_data_erm}.

\begin{figure}
\centering
\scalebox{\tikzScale}{\adjustTikzSize \input{img/TopicExplorer_data_erm.tex}}
\caption[TopicExplorer input data model]{TopicExplorer input data model. Japanese blog entries are tokenized, lemmatized and part-of-speech tagged and stored together with additional meta data.}\label{fig:TopicExplorer_data_erm}
\end{figure}

\begin{figure}
\centering
\scalebox{0.7}{\adjustTikzSize \input{img/TopicExplorer_integrated_erm.tex}}
\caption[Integrated TopicExplorer data model]{Integrated TopicExplorer data model. The general LDA ERM template (Figure~\ref{fig:topic_erm_good_reduced}) is integrated into the text input data model (Figure~\ref{fig:TopicExplorer_data_erm}).}\label{fig:TopicExplorer_integrated_erm}
\end{figure}

\subsubsection{Analysis and Visualization}
\label{subsec:casestudy_analysis}

To analyze the topic structure of given Japanese blog entries, the application programmer selects the LDA ERM template and matches the entities of the domain specific data model. Figure~\ref{fig:TopicExplorer_integrated_erm} shows the integrated data model. A blog entry is treated as a document, matching tokens and words is trivial. The integrated model contains additional domain specific constructs such as frames, as well as the LDA specific ones like topics.
