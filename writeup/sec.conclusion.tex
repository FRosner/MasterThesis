\section{Conclusion}

\subsection{Summary}

We presented a data model driven architecture for rapidly developing customized machine learning applications. This architecture is based on a framework that offers probabilistic models in form of ERMs. It makes machine learning techniques more accessible to domain experts and application developers. Additionally, providing a data model view of the inference results make it easy to integrate them into domain specific data. The responsibility of dealing with various inference algorithm implementations and file formats is taken from the application programmer and passed to the framework. The framework user simply matches an ERM of his desired probabilistic model with his domain data model and the inference results are stored in the integrated database. Combined analysis is simple and can partly be precalculated.

We developed a mapping from graphical models in plate notation to ERMs that allows framework developers to easily incorporate their probabilistic model into the set of available models. They can either provide a custom inference algorithm implementation or use generic ones that work with arbitrary graphical models. We showed that this mapping produces well-formed ERMs that aid the understanding of the probabilistic model.

\subsection{Problems and Drawbacks}

The major drawback of this approach is that the utility of the framework still heavily depends on machine learning experts. Although domain experts and applications programmers do not have to cope with probabilistic models directly, there have to be specialists who add these methods to the framework. So the success of the data model driven approach stands and falls with the data science community that supplies algorithms for the framework. Updates in existing inference algorithm implementations might need to be included manually.

Another problem arises in the quality of the supplied ERMs. In order to be understandable, they need to have meaningful entity class names. Besides that, having names for relationships and association entities that are not automatically generated is also desirable. The hidden Markov model ERM in Figure~\ref{fig:appendix_hmm}, Appendix~C for example contains an association entity ``H-O'', which represents the emission of observed states by hidden ones. Calling it ``emits'' aids the understanding.

Additionally, there are issues concerning the translation procedure of plate models to ERMs. First, models not formulated in plate notation would either need to be converted to it or the ERM needs to be obtained manually. Secondly, the loss of probabilistic dependencies might disturb users familiar with machine learning, because it does not allow to find out more about the underlying probabilistic model. One way to cope with it would be an advanced view which includes the probabilistic components. Thirdly, the current procedure does not work well with categorical variables that are not stored as a 1-of-K coded vector. The translator either has to make sure all categorical variables are 1-of-K coded or the mapping needs to be extended to work with multi-valued variables and still produce the same ERMs. [TODO: add this information to the place where the translation of 1-of-K coded variables happens?]

Difficulties might occur when there are several models for one application that slightly differ, for example by having a Bayesian prior for one parameter but the rest of the model being identical. As such details lie in the freedom of the modeling person there might be a lot of similar looking models and framework users might not be sure which one to choose. A possible way to deal with this would be to always use a fully Bayesian approach and show the details about the prior distributions in an advanced settings section. In that case the hyper-parameters could be hidden from the user in the simple data view.

Finally we would like to mention that although when using the framework, technically no machine learning specialists or statisticians are required to build an application that utilizes probabilistic models, they might still come in handy when it comes to model selection and validation. Identifying the implications of violated assumptions or detecting formal errors in applying the model to specific data is an important task which yet cannot be done in an automated manner.

\subsection{Extensions and Future Work}

The next step is to implement a prototype of the framework and to conduct experiments on usability and performance. The prototype should contain basic models for common tasks in machine learning such as clustering, classification and regression. Afterwards, the framework can be refined and more models can be added. By choosing a plug-in module based design, it would be easy for external developers to add their own models.

An automated translation of plate models to ERMs might simplify the addition of new models, but requires to formulate the probabilistic model in a machine-readable format like JavaScript object notation (JSON) or extensible markup language (XML). Investigations whether the mapping is deterministic or depends how the plates are drawn, especially for overlapping ones, would be beneficial.

The mapping itself can be refined by choosing more exact cardinalities. Relationship cardinalities are often determined by the index set cardinalities of the probabilistic variables. The self relationship of a $|V| \times |V|$ matrix will have an exact cardinality of $(|V|, |V|)$. However, the use of such tightly bound cardinalities is restricted by the underlying physical model as it can be hard to enforce specific maximal cardinalities or minimal cardinalities greater than 1. [TODO: footnote, explanation or citation why? Is it true? Mail to Brass?]

To be more flexible, we could extend the translation mechanism to work with different modeling languages. On the one hand, mapping rules for other probabilistic modeling languages could be examined. On the other hand, it might be useful to support a variety of database modeling languages like the relational model (\cite{codd1970relational}), Barker's ERM notation (\cite{barker1990case}) or UML class diagrams (\cite{rumbaugh2004unified}).
