\section{Conclusion}

We presented a data model driven architecture for rapidly developing customized machine learning applications. This architecture is based on a framework that offers probabilistic models in form of ERMs. This makes the machine learning techniques more accessible to domain experts and application developers. Additionally, providing a data model view of the inference results make it easy to integrate them into domain data. The responsibility of dealing with various inference algorithm implementations and file formats is taken from the application programmer and passed to the framework. The framework user simply matches an ERM of his desired probabilistic model with his domain data model and the inference results are stored in the integrated database. Combined analysis is simple and can partly be precalculated.

We developed a mapping from graphical models in plate notation to ERMs that allows framework developers to easily incorporate their probabilistic model into the set of available models. They can either provide a custom inference algorithm implementation or use generic ones that work with arbitrary graphical models. We show that this mapping produces well-formed ERMs that aid the understanding of the probabilistic model. [TODO: maybe state this explicitly in some place]

The major drawback of this approach is that the utility of this framework still heavily depends on machine learning experts. Although domain experts and applications programmers do not have to cope with probabilistic models directly, there has to be a specialist that adds these methods to the framework. So the success of the data model driven approach stands and falls with the data science community that supplies algorithms for the framework. Additionally, updates in existing inference algorithm implementations need to be included manually if the interface changes.

\textbf{Problems}
\begin{itemize}
\item Models that are not formulated in plate notation would either need to be converted to it or the ERM needs to be obtained in a different way.
\item So many different models possible; bayesian / non-bayesian versions $\rightarrow$ either have all of them or only bayesians and choose uninformed priors; but bayesian stuff in gaussian mixture model may be confusing for ppl familiar with k-means
\item probabilistic relationships not captured anymore
\item categorical variables need to be 1-of-K coded or if not, procedure needs to capture dimensionality of categorical variables [TODO: add this information to the place where the translation of 1-of-K coded variables happens]
\item meaningful names in ER diagrams important (e.g. emission instead of H-O in HMM, name relationships)
\item although not needed when customizing and applying the probabilistic models, machine learning scientist might still be useful to detect formal issues like violated assumptions
\end{itemize}

\textbf{Extensions and Future Work}
\begin{itemize}
\item implement framework
\item translate more models
\item analyze different types of models (not only unsupervised)
\item use min-max cardinalities given by plates (needs to be supported by data model language)
\item automate plate model to ERM translation and present it when designing models
\end{itemize}

\textbf{Why is converting vectors to entities better than adding it to the entity as vector variable?}
\begin{itemize}
\item You could put the $\mu$ vector as vector attribute to clusters or add them as columns (problem with different sized vectors, e.g. documents of different lengths)
\item 1st normal form harmed, but not that bad because there are not update anomalies
\item reusable
\item extensible
\item efficient storage can be done in physical design rather than logical (ERM)
\end{itemize}

