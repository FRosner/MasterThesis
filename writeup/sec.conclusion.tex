\section{Conclusion}

\subsection{Summary and Contribution}

We presented a data model driven architecture for rapidly developing customized machine learning applications. This architecture is based on a framework that offers probabilistic models in form of ERMs. It makes machine learning techniques more accessible to domain experts and application developers. Additionally, providing a data model view of the inference results makes it easy to integrate them into domain specific data. The responsibility of dealing with various inference algorithm implementations and file formats is taken from the application programmer and passed to the framework. The analyst simply matches an ERM of his desired probabilistic model with his domain data model and the inference results are stored in the integrated database. Combined analysis is simple and can partly be precalculated.

We developed a mapping from graphical models in plate notation to ERMs that allows framework developers to easily incorporate their probabilistic model into the set of available models. They can either provide a custom inference algorithm implementation, or use generic ones that work with arbitrary graphical models. We showed that this mapping produces well-formed ERMs which aid the understanding of the translated probabilistic models.

Compared to the current state of the art, our data model driven approach stands out for three reasons: First, it is suitable for technical and non-technical users, because it operates on ERMs. Secondly, it allows easy meta data integration to build rich, customized machine learning applications. Thirdly, by presenting a data model view of available inference algorithms, no knowledge in probabilistic modeling is required to understand the inference results. While some of the existing solutions use GUIs to be more accessible to non-technical users, they have shortcomings in terms of intuitive data integration.

\subsection{Limitations and Drawbacks}

The major drawback of this approach is that the utility of the framework still heavily depends on machine learning experts. Although domain experts and applications programmers do not have to cope with probabilistic models directly, there have to be specialists who add these methods to the framework. So the success of the data model driven approach relies on the data science community that supplies algorithms for the framework. Updates in existing inference algorithm implementations might need to be included manually if the programming interfaces change.

Another problem arises in the quality of the supplied ERMs. In order to be coherent, they need to have meaningful entity class names. Besides that, it is also desirable to have names for relationships and association entities that are not automatically generated. The hidden Markov model ERM in Figure~\ref{fig:appendix_hmm}, Appendix~C, for example, contains an association entity ``H-O'', which represents the emission of observed states by hidden ones. Calling it ``emits'' aids the understanding.

Additionally, there are issues concerning the translation procedure of plate models to ERMs. First, models not formulated in plate notation would either need to be converted to it or the ERM needs to be obtained manually. Secondly, the loss of probabilistic dependencies might disturb users familiar with machine learning, because it does not allow to find out more about the underlying probabilistic model. In some cases like a linear Regression model (Figure~\ref{fig:bayesian_regression}, Appendix B) this may affect the understanding, even for normal users. In this case the only relationships that exist are of a probabilistic nature and thus the ERM looks rather fragmented. One way to cope with this problem would be an advanced view which includes the probabilistic components. Thirdly, the current procedure does not work well with categorical variables that are not stored as a 1-of-K coded vector. The translator either has to make sure all categorical variables are 1-of-K coded or the mapping needs to be extended to work with multi-valued variables and still produce the same ERMs.

Difficulties might occur when there are several models for one application that slightly differ, for example by having a Bayesian prior for one parameter but the rest of the model being identical. As such details lie in the freedom of the modeling person, there might be a lot of similar looking models and framework users might not be sure which one to choose. A possible way to deal with this would be to always use a fully Bayesian approach and only show the details about the prior distributions in an advanced settings section. In that case the hyper-parameters could be hidden from the user in the simple data view.

A question of more theoretical nature is about the way customizing is accomplished. The probabilistic model ignores all meta data and they are not used until the final analysis step happens. What are the implications of performing inference on a simple model and integrating meta data afterwards? Experiments should be conducted to evaluate this approach of customization.

Finally, we would like to mention that although when using the framework, technically no machine learning specialists or statisticians are required to build an application that utilizes probabilistic models, they might still come in handy when it comes to model selection and validation. Identifying the implications of violated assumptions or detecting formal errors in applying the model to specific data is an important task which yet cannot be done in an automated manner.

\subsection{Extensions and Future Work}

The next step is to implement a prototype of the framework and to conduct experiments on usability and performance. The prototype should contain basic models for common tasks in machine learning such as clustering, classification and regression. Afterwards, the framework can be refined and more models can be added. By choosing a plug-in module based design, it would be easy for external developers to add their own models.

An automated translation of plate models to ERMs might simplify the addition of new models, but requires to formulate the probabilistic model in a machine-readable format like JavaScript object notation (JSON) or extensible markup language (XML). Investigations whether the mapping is deterministic or depends on how the plates are drawn, especially for overlapping ones, are be beneficial.

The mapping itself can be refined by choosing more exact cardinalities. Relationship cardinalities are often determined by the index set cardinalities of the probabilistic variables. The self relationship of a $|V| \times |V|$ matrix will have an exact cardinality of $(|V|, |V|)$. However, the use of such tightly bound cardinalities is restricted by the underlying physical model as it can be hard to enforce specific maximum cardinalities or minimum cardinalities greater than 1.

To be more flexible, we could extend the translation mechanism to work with different modeling languages. On the one hand, mapping rules for other probabilistic modeling languages could be examined. On the other hand, it might be useful to support a variety of database modeling languages like the relational model (\cite{codd1970relational}), Barker's ERM notation (\cite{barker1990case}) or UML class diagrams (\cite{rumbaugh2004unified}).

It is still an open question how to implement the data model driven framework. We could either develop it as a stand-alone web or desktop application, or create a plug-in to existing software like RapidMiner\footnote{\url{http://rapidminer.com/}}, which already offers a broad set of algorithms and a powerful GUI. Either way, it is important to provide an easy to use and simple user interface that allows intuitive matching of translated probabilistic model and domain specific ERMs. If the software also supports different execution engines and physical layers, it will be a true enrichment for every domain expert in need for customized data analysis.
