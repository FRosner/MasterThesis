\section{Conclusion}

\subsection{Summary}

We presented a data model driven architecture for rapidly developing customized machine learning applications. This architecture is based on a framework that offers probabilistic models in form of ERMs. This makes the machine learning techniques more accessible to domain experts and application developers. Additionally, providing a data model view of the inference results make it easy to integrate them into domain data. The responsibility of dealing with various inference algorithm implementations and file formats is taken from the application programmer and passed to the framework. The framework user simply matches an ERM of his desired probabilistic model with his domain data model and the inference results are stored in the integrated database. Combined analysis is simple and can partly be precalculated.

We developed a mapping from graphical models in plate notation to ERMs that allows framework developers to easily incorporate their probabilistic model into the set of available models. They can either provide a custom inference algorithm implementation or use generic ones that work with arbitrary graphical models. We show that this mapping produces well-formed ERMs that aid the understanding of the probabilistic model. [TODO: maybe state this explicitly in some place]

\subsection{Problems and Drawbacks}

The major drawback of this approach is that the utility of this framework still heavily depends on machine learning experts. Although domain experts and applications programmers do not have to cope with probabilistic models directly, there has to be a specialist that adds these methods to the framework. So the success of the data model driven approach stands and falls with the data science community that supplies algorithms for the framework. Additionally, updates in existing inference algorithm implementations might need to be included manually.

Another problem arises in the quality of the supplied ERMs. In order to be understandable, they need to have meaningful entity class names. Besides that, having names for relationships and association entities that are not automatically generated is also desirable. The hidden Markov model ERM in Figure~\ref{fig:appendix_hmm}, Appendix~C for example contains an association entity ``H-O'', which represents the emission of observed states by hidden ones. Calling it ``emits'' might aid the understanding.

Additionally, there are issues concerning the translation procedure of plate models to ERMs. First, models not formulated in plate notation would either need to be converted to it or the ERM needs to be obtained manually. Secondly, the loss of probabilistic dependencies might disturb users familiar with machine learning, because it does not allow to find out more about the underlying probabilistic model. One way to cope with it would be an advanced view which includes the probabilistic components. Thirdly, the current procedure does not work well with categorical variables that are not stored as a 1-of-K coded vector. One either has to make sure all categorical variables are 1-of-K coded or the mapping needs to be extended to work with multi-valued variables and still produce the same ERMs. [TODO: add this information to the place where the translation of 1-of-K coded variables happens?]

\textbf{Problems}
\begin{itemize}
\item although not needed when customizing and applying the probabilistic models, machine learning scientists might still be useful to detect formal issues like violated assumptions
\end{itemize}
Difficulties might occur when there are several models for one application that slightly differ, for example by having a Bayesian prior for a parameter. As this lies in the freedom of the modeling person there might be a lot of similar looking models and the users might not be sure which one to choose. A possible way to deal with this would be to always use a fully Bayesian approach and hide the details about the prior distributions in an advanced settings section. In that case the hyper-parameters could be hidden from the user in the simple view.


\subsection{Extensions and Future Work}

\textbf{Extensions and Future Work}
\begin{itemize}
\item implement framework
\item extend translation to other modeling languages
\item translate more models
\item analyze different types of models (not only unsupervised)
\item use min-max cardinalities given by plates (needs to be supported by data model language)
\item automate plate model to ERM translation and present it when designing models
\end{itemize}

\textbf{Why is converting vectors to entities better than adding it to the entity as vector variable?}
\begin{itemize}
\item You could put the $\mu$ vector as vector attribute to clusters or add them as columns (problem with different sized vectors, e.g. documents of different lengths)
\item 1st normal form harmed, but not that bad because there are not update anomalies
\item reusable
\item extensible
\item efficient storage can be done in physical design rather than logical (ERM)
\end{itemize}

