\section{Introduction}

With the growing amount of data, the need for analyzing them grows as well. \textcite{forum2012bigdata} states that data from individuals, the public and private sector are a valuable asset. Analyzed by data mining and machine learning techniques they can help to track and response to disease outbreaks, understand crisis behaviour change, accurately map service needs or predict demand and supply changes. Business is evolving to be data driven.

However, the flood of data is not accompanied by a flood of data scientists, machine learners and statisticians. Only a few specialists working at universities and research laboratories are developing the techniques needed to gain valuable insights from data. Those people are rare and expensive and not every company is able to hire such a specialist for its individual needs. Additionally, the input data heavily depends on the application domain ranging from text mining tasks, gene analysis over pattern recognition in images or speech to geographical data. Usually there is a domain expert that wants to gain insight using the available data and some application developer that is implementing the data analysis process.

We identify three roles in the traditional data analysis / machine learning application environment: (1) domain expert, (2) application programmer and (3) machine learning specialist. Aim of this work is to propose a data model driven architecture that enables application programmers and domain experts to easily customize standard machine learning algorithms and techniques to their individual problem without having to hire a machine learning expert. This architecture model is based on a framework which offers a set of generic implementations for common machine learning tasks in form of simple data models. These data models hide the complex details about the probabilistic process and focus on the results. The domain expert selects a data model template that fits the domain specific data model and the framework does the remaining work. Meta data not used by the probabilistic model is incorporated after the statistical inference. Although this approach corresponds to a simpler model with independence assumptions, it is a simple but effective way for customization.

We present a technique to build an entity relationship model (ERM) from any probabilistic model in plate notation. This is required to transform the probabilistic model formulation given in scientific publications to data models like ERMs. The concept of such data models is well known to application programmers and easy to explain to domain experts. Additionally, ERMs have the advantage that they are able to capture the structured and hierarchical nature of data, while probabilistic models like Bayesian networks become verbose very quickly.

We discuss how our data model driven approach surpasses the traditional architecture in terms of usability, comprehensibility and flexibility for the end users, that are typically domain experts and application programmers.

The thesis is structured as follows. First, we give an overview of the data model driven architecture and how it differs from the traditional approach. In the next section we briefly introduce the concept of probabilistic modeling and inference, mainly using directed graphical models in plate notation (plate models). The fourth section describes the integration of those plate models and domain specific ERMs by translating the plate models to ERMs and matching both ERMs. The fifth section provides a quick insight what different analysis can be done with the integrated model. The last section offers a complete case study of a real world application to show how the techniques from this work can be applied. In addition to the models translated in the main part of this work, plate model to ERM translations for common models are found in Appendix~A (Gaussian mixture model), B (Bayesian linear regression) and C (hidden Markov model).

\newpage

\section{Related Work}

Providing easy-to-use interfaces of machine learning algorithms is highly topical. The MLbase team of the University of California, Berkeley currently works on an application programming interface (API) to build distributed machine learning algorithm implementations with minimal complexity and competitive performance and scalability (\cite{sparks2013mli}). They focus on closing the gap between an algorithm prototype written by a scientist and the final product created by software engineers.

The Hazy project (\cite{kumar2013hazy}) tries to provide programming and infrastructure abstractions to enable a user to quickly build new systems. Goal is to integrate statistical processing techniques with data processing systems. MADlib (\cite{hellerstein2012madlib}) is an analytics library for relational database systems built on structured query language (SQL) statements.

[TODO: More related work like MLbase MLI]

Traditional machine learning algorithm libraries like Weka (\cite{hall2009weka}) and scikit-learn (\cite{scikit-learn}) use a graphical user interface to enable users to quickly select and try different algorithms. The problem is that they do not offer an easy way to integrate the results with domain specific meta data and offer only a textual explanation of what the algorithms do. Our data model driven approach allows better data integration and visualizes how the inference results look like.

However, all these approaches do not provide such an easy way to select and understand inference algorithms and to integrate existing meta data into the analysis like our approach does.
