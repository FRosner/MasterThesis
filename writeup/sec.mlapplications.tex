\section{Machine Learning Applications}

This section gives an overview of two machine learning application architectures: The traditional one and the data model driven architecture. We compare both approaches, identify the roles involved in the development process and challenges that arise from choosing either architecture.

\subsection{Roles}

In a typical machine learning application there is a \emph{domain expert} who has data available and a problem that needs to be solved. Common tasks are exploratory analysis or hypothesis testing (\cite{tukey1980we}). A popular technique to perform such analysis is to use probabilistic models. By selecting a model that fits the given data one can then use an inference algorithm to calculate values of hidden variables. Those variables correspond to some hidden structure of the data that is of interest for the analyst.

Most of the time the domain expert will either have an \emph{application programmer} at hand or do the work himself. The application programmer is responsible for implementing the analysis pipeline, plugging the data and performing/preparing the analysis.

\emph{Machine learning specialists} are responsible for developing and evaluating new models for different applications. They often have high theoretical knowledge in the field of statistics and computer science and present new models in scientific publications.

Often, neither domain experts nor application programmers have the required knowledge to develop and implement a custom model for the given problem and machine learning specialists are rare and expensive. Luckily, there are a couple of standard implementations for common probabilistic models available.\footnote{Section~\ref{subsec:custom-inference} gives a broader overview of existing inference algorithm implementations.} However, it can be difficult to customize those implementations and to include the available meta data into the analysis. A common practice is to use standard models and join the meta data afterwards to perform analysis on the integrated database.

\subsection{Traditional Architecture}

\begin{figure}[t]
\centering
\scalebox{\tikzScale}{\adjustTikzSize \input{img/MLApplication_typical.tex}}
\caption[Traditional machine learning application architecture]{Traditional machine learning application architecture.}\label{fig:ml-application-architecture}
\end{figure}

The traditional architecture is visualized in Figure~\ref{fig:ml-application-architecture}. In step (A), the data are plugged into an inference algorithm implementation, which (B) outputs a parameter estimate or some hidden variable values. In (C), the results are combined with the data to be available for further analysis, which happens in (D). Afterwards the results will be presented.

Here the domain expert provides the data and the problem to solve. He may select a model on his own or with the help of a machine learning expert. He will analyze and interpret the results. The application programmer then has to select a standard implementation of the desired probabilistic model together with an inference algorithm. He must implement an interface to plug in the data in (A) and has to understand the output of the inference algorithm in (B). This can sometimes be difficult to do, especially if not all required information are provided.\footnote{In a Bayesian approach parameters are often marginalized over. An implementation using collapsed Gibbs sampling (\cite{liu1994collapsed}) like the topic modeling component of the machine learning for language toolkit (\cite{mccallum2002mallet}) might not output a point estimate for those parameters.} The application programmer needs to understand the probabilistic model to calculate the missing parameters and variables. In (C), the results of the inference algorithm need to be combined with the original data. Although this can be done easily by reading the documentation of the implementation it can be quite tedious to cope with different file formats and sometimes very abstract variable names. The analysis in (D) is straight forward when (C) is completed successfully. Typical analysis tasks are group-by aggregations, transformations and filters on both original and inferred data.

All the tasks from (A) to (D) require different knowledge, often related to statistical modeling, that may not be available to the application programmer or domain expert. The traditional architecture suffers from the lack of a machine learning expert and does not allow to rapidly develop customized machine learning applications by ordinary software developers. Our data model driven architecture on the other hand enables any person that is familiar with simple data models like ERMs to build a customized machine learning application without touching any complex mathematical equations or probabilistic models at any time.

\subsection{Data Model Driven Architecture}

\begin{figure}
\centering
\scalebox{\tikzScale}{\adjustTikzSize \input{img/MLApplication_dmdriven.tex}}
\caption[Data model driven machine learning application architecture]{Data model driven machine learning application architecture.}\label{fig:ml-application-architecture-dm}
\end{figure}

The data model driven architecture transforms most of the problems stated above into working with data models. The architecture is shown in Figure~\ref{fig:ml-application-architecture-dm}. It requires a framework that hides all probabilistic and mathematical details behind simple data models. By translating probabilistic models into ERMs\footnote{The translation process has to be done by the framework developer. It is described further in Section~\ref{sec:pm2erm}} the problem of data integration (C) is moved to a higher abstraction level. Instead of coping with files and complex mathematical formulas, the analyst can simply select a probabilistic model by integrating its data model into the domain data model.\footnote{Section~\ref{sec:erm_matching} goes into detail about matching both models.}

This moves the integration step (C) being the first step in the development process. Selecting an inference algorithm, plugging the input data into it and reading the results (A + B) is done entirely by the framework. This is possible because the framework offers a fixed data model which the inference results match. The inference result data are simply included into the schema, depending on the underlying architecture, e.g. as SQL statements into a relational database.

This leaves the analyst only with the analysis on the integrated data (D). However, as the data model template of each probabilistic model is known it is possible to provide a set of standard analysis tasks based on the inference results.

Thus compared to the traditional architecture, the data model driven one enables domain experts and application programmers to use probabilistic models and customize them by integrating application specific meta data. No statistical knowledge is required, only the understanding of data models like ERMs and/or the relational model. Using ERMs allows to use different implementations depending on the desired infrastructure. The framework can operate on relational databases, file level or in a distributed environment.
