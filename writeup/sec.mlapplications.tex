\section{Machine Learning Applications}

This section gives an overview of two machine learning application architectures: The traditional one and the data model driven architecture. We compare both approaches, identify the roles involved in the development process and challenges that arise from choosing either architecture. We show that our data model driven architecture is superior in terms of [TODO: superior in what?].

\subsection{Roles}

In a typical machine learning application there is a \emph{domain expert} who has data available and a problem that needs to be solved. Common tasks are exploratory analysis or hypothesis testing [TODO: cite statistics book]. A popular technique to perform such analysis is to use probabilistic models. By selecting a model that fits the given data one can then use an inference algorithm to calculate values of hidden variables. Those variables correspond to some hidden structure of the data that is of interest for the analyst.

Most of the time the domain expert will either have an \emph{application programmer} at hand or do the work himself. The application programmer is responsible for implementing the analysis pipeline, plugging the data and performing/preparing the analysis.

\emph{Machine learning specialists} are responsible for developing and evaluating new models for different applications. They often have high theoretical knowledge in the field of statistics and computer science and present new models in scientific publications.

Often, neither domain experts nor application programmers have the required knowledge to develop and implement a custom model for the given problem and machine learning specialists are rare and expensive. Luckily, there are a couple of standard implementations for common probabilistic models available.\footnote{Section~\ref{subsec:custom-inference} gives a broader overview of existing inference algorithm implementations.} However, it can be difficult to customize those implementations and to include the available meta data into the analysis. A common practice is to use standard models and join the meta data afterwards to perform analysis on the integrated database.

\subsection{Traditional Architecture}

\begin{figure}[t]
\centering
\scalebox{\tikzScale}{\adjustTikzSize \input{img/MLApplication_typical.tex}}
\caption{Traditional machine learning application architecture.}\label{fig:ml-application-architecture}
\end{figure}

The traditional architecture is visualized in Figure~\ref{fig:ml-application-architecture}. In step (A), the data are plugged into an inference algorithm implementation, which (B) outputs a parameter estimate or some hidden variable values. In (C), the results are combined with the data to be available for further analysis, which happens in (D). Afterwards the results will be presented.

Here the domain expert provides the data and the problem to solve. He may select a model on his own or with the help of a machine learning expert. He will analyze and interpret the results. The application programmer then has to select a standard implementation of the desired probabilistic model together with an inference algorithm. He must implement an interface to plug in the data in (A) and has to understand the output of the inference algorithm in (B). This can sometimes be difficult to do as most implementations are highly optimized and do not necessarily output all required information [TODO: was wird wegoptimiert? collapsed Gibbs sampling? CITATION NEEDED]. The application programmer needs to understand the probabilistic model to calculate the missing parameters and variables. In (C), the results of the inference algorithm need to be combined with the original data. Although this can be done easily by reading the documentation of the implementation it can be quite tedious to cope with different file formats and sometimes very abstract variable names. The analysis in (D) is straight forward when (C) is completed successfully. Typical analysis tasks are group-by aggregations, transformations and filters on both original and inferred data.

All the tasks from (A) to (D) require different knowledge, often related to statistical modeling, that may not be available to the application programmer or domain expert. The traditional architecture suffers from the lack of a machine learning expert and does not allow to rapidly develop customized machine learning applications by ordinary software developers. Our data model driven architecture on the other hand enables any person that is familiar with simple data models like ERMs to build a customized machine learning application without touching any complex mathematical equations or probabilistic models at any time.

\subsection{Data Model Driven Architecture}

\begin{figure}
\centering
\scalebox{\tikzScale}{\adjustTikzSize \input{img/MLApplication_dmdriven.tex}}
\caption{Data model driven machine learning application architecture.}\label{fig:ml-application-architecture-dm}
\end{figure}

The data model driven architecture transforms most of the problems stated above into working with data models. The architecture is shown in Figure~\ref{fig:ml-application-architecture-dm}. It requires a framework that hides all probabilistic and mathematical details behind simple data models. By translating probabilistic models into ERMs\footnote{The translation process has to be done by the framework developer. It is described further in Section~\ref{sec:pm2erm}} the problem of data integration is moved to a higher abstraction level. Instead of coping with files and complex mathematical formulas, the analyst can simply select a probabilistic model by integrating its data model into the domain data model.\footnote{Section~\ref{sec:erm_matching} goes into detail about matching both models.}

This moves the integration step (C) being the first step in the development process. Selecting an inference algorithm, plugging the input data into it and reading the results is done entirely by the framework. This is possible because the framework offers a fixed data model which the inference results match. The inference result data are simply included into the schema, depending on the underlying architecture, e.g. as SQL statements into a relational database.

This leaves the analyst only with the analysis on the integrated data (D). However, as the data model template of each probabilistic model is known it is possible to provide a set of standard analysis tasks based on the inference results.

---

\textbf{Roles}
\begin{itemize}
\item Traditional roles
	\begin{itemize}
	\item Domain expert (provides data, does hypothesis testing using analysis result and visualization)
	\item Machine learning scientist (provides inference algorithm, data analysis, transformation and visualization)
	\end{itemize}
	$\Rightarrow$ Problem: Machine learning scientists are rare and expensive
\item Solution: New roles
	\begin{itemize}
	\item Domain expert (provides data, does hypothesis testing using analysis result and visualization)
	\item Application programmer (uses standard inference algorithm implementation, can perform arbitrary analysis on data and inference results using simple queries, provides visualization)
	\end{itemize}
	$\Rightarrow$ Machine learning scientist provides standard inference algorithm implementations
\end{itemize}

\textbf{Challenges}
\begin{enumerate}
\item[(A)] Data needs to be converted to the required inference algorithm input format
	\begin{description}
	\item[Traditional solution] Consult the documentation of the inference algorithm implementation and convert the data into the required input format
	\item[New solution] Conversion under the hood given the matched data model because the implementation is known
	\end{description}
\item[(B)] Optimized implementations may not explicitly calculate all missing variables
	\begin{description}
	\item[Traditional solution] Manually calculating the missing values and distributions
	\item[New solution] Application of sum-product and max-sum algorithm on the given results and the model
	\end{description}
\item[(C)] Integration of given data and inference results
	\begin{description}
	\item[Traditional solution] Consult the documentation of the inference algorithm output format, understand it and load it back to your original data integrating it manually
	\item[New solution] Data integration by model integration: match the statistical model and the model of the given data so that integration is just a matter of writing data corresponding to the combined schema. Output format is known by the framework developer.
	\end{description}
\item[(D)] Analysis of data having partly probabilistic relationships (common queries)
	\begin{description}
	\item[Traditional solution] Calculating group-by aggregations, filters and transformations (sometimes representing computing marginals on filtered data $\rightarrow$ normalization required)
	\item[New solution] POLAP offering common transformation operations on entities and normal or probabilistic relationships.
	\end{description}
\end{enumerate}
