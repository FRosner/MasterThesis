\section{Probabilistic Models}

Probabilistic models are a mighty and flexible technique to model and reason about the real world. They are known to statisticians, machine learners and data miners alike. However, different target groups [TODO: find different word for Zielgruppen] yield different modeling languages. The following sections present common probabilistic modeling languages and how to use them to perform statistical inference.

\subsection{Probabilistic Modeling Languages}

Over the past decades, a variety of different languages have been used and developed. Despite the most general one, using mathematical formulas only, we want to introduce graphical representations that often make it easier to understand the relationships between objects and what independence assumptions have been made. Starting with a very popular one called \emph{directed graphical model}, or \emph{Bayes net}, that are going to be described in detail and used later in this thesis, the second part of this subsection will give a quick overview of other alternatives.

\subsubsection{Directed Graphical Models}

\begin{itemize}
\item aka Bayesian networks
\item factorization of joint distribution in product of conditionals
\item variables = circles
\item dependencies = arrows / independence assumptions = lack of arrows
\item repeated variables = plates
\item shaded circles = observed variables [CITATION NEEDED (Bishop?)]
\item small dark circles = parameters [CITATION NEEDED (Bishop?)]
\item state difference between parameters and hidden variables?
\end{itemize}

\textbf{Plate models}
\begin{itemize}
\item developed by \textcite{buntine1994operations} / \textcite{gilks1994language}, now common in graphical models ([CITATION NEEDED] for graphical models, esp. baysian networks)
\item plates for repeated variables of the same type
\end{itemize}

\textbf{Problems with traditional plate models}
\begin{itemize}
\item implicit constraints (\cite{heckerman2007probabilistic})
\end{itemize}

\begin{Example} Plate model for Dirichlet multinomial text clustering
\label{ex:clustering-pm}

A typical way to model text data is to model documents as a list of independent tokens, in a so called \emph{bag-of-words} model (\cite{murphy2012machine},~p.~87-89). It has been found that modeling the word counts per document using a multinomial distribution yields better classification performance than using a Bernoulli model (\cite{mccallum1998comparison}). Depending on if there is a supervised or unsupervised scenario, this model can be used for classification or clustering, respectively.\footnote{When the cluster assignments $\vec z_n$ are known (supervised learning), the inference of the parameters is exact. The parameter estimations can be used to classify new documents. If the cluster assignments are unknown (unsupervised learning), they also have to be inferred using approximate inference. More on inference in probabilistic models can be found in Section~\ref{subsec:inference}.} For this thesis we shall use a Bayesian approach of this bag-of-words model for clustering documents as a universal example. As it uses Dirichlet distributed priors for $\vec \mu_k$ and $\vec \pi$, it is called \emph{Dirichlet multinomial clustering}.

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/clustering_transformation_platemodel.tex}}
	\end{center}
\caption[Plate model for Dirichlet multinomial clustering of text documents]{Plate model for Dirichlet multinomial clustering of text documents.}
		\label{fig:clustering_platemodel}
\end{figure}

Given a collection of documents $N$, where each document $n \in N$ consists of a set of tokens $M_n$, each document is assigned to a cluster $k \in K$. $\vec z_{n}$ is a 1-of-K coded variable (\cite{bishop2006pattern},~p.~424) having a 1 at position $k$ if the document $n$ is assigned to cluster $k$. Each cluster has its own word distribution parameterized by $\vec \mu_k$. The plate model is shown in Figure~\ref{fig:clustering_platemodel}.

The variables have the following constraints:
\begin{align}
\sum_{k \in K} z_{nk} &= 1,\quad \text{a document is assigned to exactly one cluster}\\
\sum_{v \in V} d_{nmv} &= 1,\quad \text{a token is of exactly one word type}\\
\sum_{k \in K} \pi_{k} &= 1,\quad \text{the cluster proportions sum to one}\\
\sum_{v \in V} \mu_{kv} &= 1,\quad \text{the word distribution per cluster sums to one}
\end{align}

The generative process is as follows:
\begin{enumerate}
\item draw cluster proportions $\vec \pi \sim \text{Dir}(\vec \alpha)$
\item draw word proportions per cluster $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$
\item draw cluster assignments per document $\forall n \in N: \vec z_n \sim \text{Mult}(\vec \pi)$
\item draw word type per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the cluster selected by $\vec z_n$
\end{enumerate}

\end{Example}

\subsubsection{Other Probabilistic Modeling Languages}

\begin{description}
\item[Mathematical formulas only]\ 
\begin{itemize}
\item Express joint probability factorization with only formulas
\item quick and effective way
\item may be hard to talk about it and hard to understand when not supported by graphical representation
\item can and should be combined with graphical notations, because it is the most exact one
\end{itemize}

\item[Directed Graphical Models]\ 
\begin{itemize}
\item aka Markov random fields
\item similar to Bayesian networks but with undirected edges
\item factorization of joint distribution in normalized product of factors, one per maximal clique
\end{itemize}

\item[Factor Graphs]\ 
\begin{itemize}
\item developed by \textcite{kschischang2001factor} and extended by \textcite{frey2002extending}
\item generalize directed and undirected graphical models
\item bipartite graph with variables and factors
\item can naturally express inference algorithms for arbitrary graphs
\end{itemize}

\item[Probabilistic Relational Models]\ 
\begin{itemize}
\item relational database model having probabilistic relationships between attributes (\cite{friedman1999learning})
\end{itemize}

\item[Probabilistic Entity-Relationship Models]\ 
\begin{itemize}
\item extension of entity-relationship model with probabilstic relationships between attributes of entity types
\item \cite{heckerman2007probabilistic} describe directed acyclic probabilistic entity-relationship model (DAPER)
\item special focus on relationships between entities (restricted, self and probabilistic relationships)
\end{itemize}
\end{description}

\subsection{Inference on Probabilistic Models}
\label{subsec:inference}

\subsubsection{Inference Algorithms for Arbitrary Graphical Models}

\begin{itemize}
\item Belief propagation / Sum-product algorithm
\item Max-sum algorithm
\item Variational message passing
\end{itemize}

\subsubsection{Custom Inference Algorithm Implementations}
\label{subsec:custom-inference}

\begin{itemize}
\item Weka, Mallet, Mahout, Shark, Spark, ...
\item highly optimized inference algorithm implementations\\
$\Rightarrow$ instead of naive message passing algorithm like storage of all sums and stuff, the programmers know which stuff they want to use and need to store or which can be computed on the fly; it's not the update equations that differ but their implementation details
\end{itemize}

