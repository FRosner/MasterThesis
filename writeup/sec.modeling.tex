\section{Probabilistic Models}

\subsection{Probabilistic Modeling Languages}

\subsubsection{Directed Graphical Models}

\begin{itemize}
\item aka Bayesian networks
\item factorization of joint distribution in product of conditionals
\item variables = circles
\item dependencies = arrows / independence assumptions = lack of arrows
\item repeated variables = plates
\item shaded circles = observed variables [CITATION NEEDED (Bishop?)]
\item small dark circles = parameters [CITATION NEEDED (Bishop?)]
\item state difference between parameters and hidden variables?
\end{itemize}

\textbf{Plate models}
\begin{itemize}
\item developed by \textcite{buntine1994operations} / \textcite{gilks1994language}, now common in graphical models ([CITATION NEEDED] for graphical models, esp. baysian networks)
\item plates for repeated variables of the same type
\end{itemize}

\textbf{Problems with traditional plate models}
\begin{itemize}
\item implicit constraints (\cite{heckerman2007probabilistic})
\end{itemize}

\begin{Example} Plate model for Dirichlet multinomial text clustering
\label{ex:clustering-pm}

\textbf{Generative process}
\begin{enumerate}
\item draw cluster proportions $\vec \pi \sim \text{Dir}(\vec \alpha)$
\item draw word proportions per cluster $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$
\item draw cluster assignments per document $\forall n \in N: \vec z_n \sim \text{Mult}(\vec \pi)$
\item draw token assignments per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the cluster selected by $\vec z_n$
\end{enumerate}

\textbf{Constraints}
\begin{align}
\sum_{k \in K} z_{nk} &= 1\\
\sum_{v \in V} d_{nmv} &= 1\\
\sum_{k \in K} \pi_{k} &= 1\\
\sum_{v \in V} \mu_{kv} &= 1
\end{align}

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/clustering_transformation_platemodel.tex}}
	\end{center}
\caption{Plate model for Dirichlet multinomial clustering of text documents.}
		\label{fig:clustering_platemodel}
\end{figure}

\end{Example}

\subsubsection{Other Probabilistic Modeling Languages}

\begin{description}
\item[Directed Graphical Models]\ 
\begin{itemize}
\item aka Markov random fields
\item similar to Bayesian networks but with undirected edges
\item factorization of joint distribution in normalized product of factors, one per maximal clique
\end{itemize}

\item[Factor Graphs]\ 
\begin{itemize}
\item developed by \textcite{kschischang2001factor} and extended by \textcite{frey2002extending}
\item generalize directed and undirected graphical models
\item bipartite graph with variables and factors
\item can naturally express inference algorithms for arbitrary graphs
\end{itemize}

\item[Probabilistic Relational Models]\ 
\begin{itemize}
\item relational database model having probabilistic relationships between attributes (\cite{friedman1999learning})
\end{itemize}

\item[Probabilistic Entity-Relationship Models]\ 
\begin{itemize}
\item extension of entity-relationship model with probabilstic relationships between attributes of entity types
\item \cite{heckerman2007probabilistic} describe directed acyclic probabilistic entity-relationship model (DAPER)
\item special focus on relationships between entities (restricted, self and probabilistic relationships)
\end{itemize}
\end{description}

\subsection{Inference on Probabilistic Models}

\subsubsection{Inference Algorithms for Arbitrary Graphical Models}

\begin{itemize}
\item Belief propagation / Sum-product algorithm
\item Max-sum algorithm
\item Variational message passing
\end{itemize}

\subsubsection{Custom Inference Algorithm Implementations}

\begin{itemize}
\item Weka, Mallet, Mahout, Shark, Spark, ...
\item highly optimized inference algorithm implementations\\
$\Rightarrow$ [ALEX FRAGEN] was hier optimiert wird im Vergleich zu Infer.NET
\end{itemize}

