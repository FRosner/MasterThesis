\section{Probabilistic Models}

Probabilistic models are a mighty and flexible technique to model and reason about the real world. They are known to statisticians, machine learners and data miners alike. However, different target groups [TODO: find different word for Zielgruppen] yield different modeling languages. The following sections present common probabilistic modeling languages and how to use them to perform statistical inference.

\subsection{Probabilistic Modeling Languages}

Over the past decades, a variety of different probabilistic modeling languages have been used and developed. Despite the most general one, using mathematical formulas only, we want to introduce graphical representations that often make it easier to understand the relationships between objects and what independence assumptions have been made. Starting with a very popular one called \emph{directed graphical model} (DGM), or \emph{Bayesian network}, that is going to be described in detail and used later in this thesis, the second part of this subsection will give a quick overview of other alternatives.

\subsubsection{Directed Graphical Models}

DGMs are a well known and powerful graphical probabilistic modeling language (\cite{murphy2012machine}~p.~307~ff.; \cite{bishop2006pattern},~p.~360~ff.; \cite{pearl1988probabilistic},~p.~116~ff.). They express variables and independence assumptions using a directed graph. Vertices represent variables, while the absence of edges models that the two variables not connected are assumed to be independent. That means that a DGM induces a factorization of the joint probability, given by the graph structure.

Figure~\ref{fig:simple_dgm} shows a simple DGM with three variables. The final grade $G$ of an exam depends on the amount of class participation $C$ by that student and how much time $T$ he spent to prepare for the exam. The joint probability of this model factorizes into $$P(G,C,T) = P(G | C, T) \cdot P(C) \cdot P(T).$$ It is assumed that the two variables $C$ and $T$ are independent, given that $G$ is unknown. Reasoning about conditional independence can be done by using the framework of \emph{d-separation}  (\cite{lauritzen1996graphical},~p.~48~ff.; \cite{pearl1988probabilistic}).

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/simple_dgm.tex}}
	\end{center}
\caption[Simple directed graphical model containing three variables.]{Simple DGM containing three variables. The final grade of an exam is influenced by the students class participation and how much time he spent to prepare for the exam.}
		\label{fig:simple_dgm}
\end{figure}

Note that if a variable is observed, the vertex of this variable is drawn as a shaded circle. Hyper-parameters are drawn as small, black circles.\footnote{Hyper-parameters are a concept of Bayesian statistics. In a Bayesian world, the parameter of a probability distribution, for example the probability to see heads when tossing a coin, is also a random variable. It is sampled from a prior distribution having another set of parameters. Those are called hyper-parameters. Prior distributions are used to encode prior knowledge into the model and to avoid the zero count problem, similar to Laplace smoothing (\cite{manning2008introduction},~p.~260).}

\textbf{Plate models}
\begin{itemize}
\item developed by \textcite{buntine1994operations} / \textcite{gilks1994language}, now common in graphical models ([CITATION NEEDED] for graphical models, esp. baysian networks)
\item plates for repeated variables of the same type
\end{itemize}

\textbf{Problems with traditional plate models}
\begin{itemize}
\item implicit constraints (\cite{heckerman2007probabilistic})
\end{itemize}

\begin{Example} Plate model for Dirichlet multinomial text clustering
\label{ex:clustering-pm}

A typical way to model text data is to model documents as a list of independent tokens, in a so called \emph{bag-of-words} model (\cite{murphy2012machine},~p.~87-89). It has been found that modeling the word counts per document using a multinomial distribution yields better classification performance than using a Bernoulli model (\cite{mccallum1998comparison}). Depending on if there is a supervised or unsupervised scenario, this model can be used for classification or clustering, respectively.\footnote{When the cluster assignments $\vec z_n$ are known (supervised learning), the inference of the parameters is exact. The parameter estimations can be used to classify new documents. If the cluster assignments are unknown (unsupervised learning), they also have to be inferred using approximate inference. More on inference in probabilistic models can be found in Section~\ref{subsec:inference}.} For this thesis we shall use a Bayesian approach of this bag-of-words model for clustering documents as a universal example. As it uses Dirichlet distributed priors for $\vec \mu_k$ and $\vec \pi$, it is called \emph{Dirichlet multinomial clustering}.

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/clustering_transformation_platemodel.tex}}
	\end{center}
\caption[Plate model for Dirichlet multinomial clustering of text documents]{Plate model for Dirichlet multinomial clustering of text documents.}
		\label{fig:clustering_platemodel}
\end{figure}

Given a collection of documents $N$, where each document $n \in N$ consists of a set of tokens $M_n$, each document is assigned to a cluster $k \in K$. $\vec z_{n}$ is a 1-of-K coded variable (\cite{bishop2006pattern},~p.~424) having a 1 at position $k$ if the document $n$ is assigned to cluster $k$. Each cluster has its own word distribution parameterized by $\vec \mu_k$. The plate model is shown in Figure~\ref{fig:clustering_platemodel}.

The variables have the following constraints:
\begin{align}
\sum_{k \in K} z_{nk} &= 1,\quad \text{a document is assigned to exactly one cluster}\\
\sum_{v \in V} d_{nmv} &= 1,\quad \text{a token is of exactly one word type}\\
\sum_{k \in K} \pi_{k} &= 1,\quad \text{the cluster proportions sum to one}\\
\sum_{v \in V} \mu_{kv} &= 1,\quad \text{the word distribution per cluster sums to one}
\end{align}

The generative process is as follows:
\begin{enumerate}
\item draw cluster proportions $\vec \pi \sim \text{Dir}(\vec \alpha)$
\item draw word proportions per cluster $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$
\item draw cluster assignments per document $\forall n \in N: \vec z_n \sim \text{Mult}(\vec \pi)$
\item draw word type per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the cluster selected by $\vec z_n$
\end{enumerate}

\end{Example}

\subsubsection{Other Probabilistic Modeling Languages}

\begin{description}
\item[Mathematical formulas only]\ 
\begin{itemize}
\item Express joint probability factorization with only formulas
\item quick and effective way
\item may be hard to talk about it and hard to understand when not supported by graphical representation
\item can and should be combined with graphical notations, because it is the most exact one
\end{itemize}

\item[Directed Graphical Models]\ 
\begin{itemize}
\item aka Markov random fields
\item similar to Bayesian networks but with undirected edges
\item factorization of joint distribution in normalized product of factors, one per maximal clique
\end{itemize}

\item[Factor Graphs]\ 
\begin{itemize}
\item developed by \textcite{kschischang2001factor} and extended by \textcite{frey2002extending}
\item generalize directed and undirected graphical models
\item bipartite graph with variables and factors
\item can naturally express inference algorithms for arbitrary graphs
\end{itemize}

\item[Probabilistic Relational Models]\ 
\begin{itemize}
\item relational database model having probabilistic relationships between attributes (\cite{friedman1999learning})
\end{itemize}

\item[Probabilistic Entity-Relationship Models]\ 
\begin{itemize}
\item extension of entity-relationship model with probabilstic relationships between attributes of entity types
\item \cite{heckerman2007probabilistic} describe directed acyclic probabilistic entity-relationship model (DAPER)
\item special focus on relationships between entities (restricted, self and probabilistic relationships)
\end{itemize}
\end{description}

\subsection{Inference on Probabilistic Models}
\label{subsec:inference}

\subsubsection{Inference Algorithms for Arbitrary Graphical Models}

\begin{itemize}
\item Belief propagation / Sum-product algorithm
\item Max-sum algorithm
\item Variational message passing
\end{itemize}

\subsubsection{Custom Inference Algorithm Implementations}
\label{subsec:custom-inference}

\begin{itemize}
\item Weka, Mallet, Mahout, Shark, Spark, ...
\item highly optimized inference algorithm implementations\\
$\Rightarrow$ instead of naive message passing algorithm like storage of all sums and stuff, the programmers know which stuff they want to use and need to store or which can be computed on the fly; it's not the update equations that differ but their implementation details
\end{itemize}

