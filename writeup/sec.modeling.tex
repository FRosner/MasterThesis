\section{Probabilistic Models}

Probabilistic models are a mighty and flexible technique to model and reason about the real world. They are known to statisticians, machine learners and data miners alike. However, different users yield different modeling languages. The following sections present common probabilistic modeling languages and how to use them to perform statistical inference.

\subsection{Probabilistic Modeling Languages}

Over the past decades, a variety of different probabilistic modeling languages have been used and developed. Despite the most general one, using mathematical formulas only, we want to highlight graphical representations that often make it easier to understand the relationships between objects and which independence assumptions have been made. We focus on a very popular one called \emph{directed graphical model} (DGM), or \emph{Bayesian network}, and an extension called \emph{plate model} that is going to be described in detail and used extensively throughout this thesis to represent the probabilistic view of the data. The last part of this subsection will give a quick overview of other alternatives.

\subsubsection{Directed Graphical Models}

DGMs are a well known and powerful graphical probabilistic modeling language (\cite{murphy2012machine}~p.~307~ff.; \cite{bishop2006pattern},~p.~360~ff.; \cite{pearl1988probabilistic},~p.~116~ff.). They express variables and independence assumptions using a directed graph. Vertices represent variables, while the absence of edges models that the two variables not connected are assumed to be independent. That means that a DGM induces a factorization of the joint probability, given by the graph structure.

Figure~\ref{fig:simple_dgm} shows a simple DGM with three variables. The final grade $g$ of an exam depends on the amount of class participation $c$ by that student and how much time $t$ he spent to prepare for the exam. The joint probability of this model factorizes into $$P(g,c,t) = P(g | c, t) \cdot P(c) \cdot P(t).$$ It is assumed that the two variables $c$ and $t$ are independent, given that $g$ is unknown. Reasoning about conditional independence can be done by using the framework of \emph{d-separation}  (\cite{lauritzen1996graphical},~p.~48~ff.; \cite{pearl1988probabilistic}).

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/simple_dgm.tex}}
	\end{center}
\caption[Simple directed graphical model containing three variables.]{Simple DGM containing three variables. The final grade of an exam is influenced by the students class participation and how much time he spent to prepare for the exam.}
		\label{fig:simple_dgm}
\end{figure}

Following the notation of \textcite{bishop2006pattern}, vertices of observed variable are drawn as shaded circles, while hyper-parameters\footnote{Hyper-parameters are a concept of Bayesian statistics. In a Bayesian world, the parameter of a probability distribution, for example the probability to see heads when tossing a coin, is also a random variable. It is sampled from a prior distribution having another set of parameters. Those are called hyper-parameters. Prior distributions are used to encode prior knowledge into the model and to avoid the zero count problem, similar to Laplace smoothing (\cite{manning2008introduction},~p.~260).} are drawn as small, black circles. This helps to distinguish them from normal parameters and hidden variables.\footnote{Formally there is no difference between parameters and hidden variables. Both are vertices in the DGM and represent random variables, but while the number of parameters remains constant, the number of hidden variables might increase with an increasing amount of data.}

\subsubsection{Plate Models}

DGMs are natural for modeling flat data, i.e. single variables and their dependencies. When it comes to hierarchical data, traditional DGMs become verbose and hard to read very quickly. Consider a set of $s$ students, all writing the same exam. This would require to repeat the vertices from Figure~\ref{fig:simple_dgm} $s$ times to capture this repeated measurement. If one wants to model multiple exams, professors or even universities, the resulting model becomes inscrutable.

Plate models (\cite{buntine1994operations}; \cite{gilks1994language}) introduce a mechanism for a compact notation of those repeated measurements. Instead of explicitly repeating a variable $s$ times, it is covered by a plate that has the cardinality $s$ and an index is added to the variable. So for every student $s \in S$ there is a final grade $g_s$, an amount of class participation $c_s$ and an exam preparation time $t_s$. We also added an additional variable outside of the students plate, measuring the average grade of all students. As the edge from the individual grade to this aggregated variable leaves the student plate, it reflects the dependency of the average grade on the grades from all students.

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/simple_platemodel.tex}}
	\end{center}
\caption[Simple plate model containing three variables.]{Simple plate model containing three variables. The model now contains a plate to capture the repeated measurement of the variables for every student of the class. Additionally, the average of all grades is measured as another variable outside of the plate.}
		\label{fig:simple_platemodel}
\end{figure}

\textcite{heckerman2007probabilistic} state that traditional plate models require edge constraints to be more expressive. Those constraints are often implicit: In the expanded graph of Figure~\ref{fig:simple_platemodel}, there are only edges between variables inside the Students plate if they belong to the same student (have the same index $s$). Each student is statistically independent of all the others.

\begin{Example} Plate model for Dirichlet multinomial text clustering
\label{ex:clustering-pm}

A typical way to model text data is to model documents as a list of independent tokens, in a so called \emph{bag-of-words} model (\cite{murphy2012machine},~p.~87-89). It has been found that modeling the word counts per document using a multinomial distribution yields better classification performance than using a Bernoulli model (\cite{mccallum1998comparison}). Depending on if there is a supervised or unsupervised scenario, this model can be used for classification or clustering, respectively.\footnote{When the cluster assignments $\vec z_n$ are known (supervised learning), the inference of the parameters is exact. The parameter estimations can be used to classify new documents. If the cluster assignments are unknown (unsupervised learning), they also have to be inferred using approximate inference. More on inference in probabilistic models can be found in Section~\ref{subsec:inference}.} For this thesis we shall use a Bayesian approach of this bag-of-words model for clustering documents as a universal example. As it uses Dirichlet distributed priors for $\vec \mu_k$ and $\vec \pi$, it is called \emph{Dirichlet multinomial clustering}.

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/clustering_transformation_platemodel.tex}}
	\end{center}
\caption[Plate model for Dirichlet multinomial clustering of text documents]{Plate model for Dirichlet multinomial clustering of text documents.}
		\label{fig:clustering_platemodel}
\end{figure}

Given a collection of documents $N$, where each document $n \in N$ consists of a set of tokens $M_n$, each document is assigned to a cluster $k \in K$. $\vec z_{n}$ is a 1-of-K coded variable (\cite{bishop2006pattern},~p.~424) having a 1 at position $k$ if the document $n$ is assigned to cluster $k$. Each cluster has its own word distribution parameterized by $\vec \mu_k$. The plate model is shown in Figure~\ref{fig:clustering_platemodel}.

The variables have the following constraints:
\begin{align}
\sum_{k \in K} z_{nk} &= 1,\quad \text{a document is assigned to exactly one cluster}\\
\sum_{v \in V} d_{nmv} &= 1,\quad \text{a token is of exactly one word type}\\
\sum_{k \in K} \pi_{k} &= 1,\quad \text{the cluster proportions sum to one}\\
\sum_{v \in V} \mu_{kv} &= 1,\quad \text{the word distribution per cluster sums to one}
\end{align}

The joint distribution factorization is given as:
\[P(\vec d, \vec z, \vec \pi, \vec \mu | \vec \alpha, \vec \beta) = P(\vec d | \vec z, \vec \pi) \cdot P(\vec z | \vec \pi) \cdot P(\vec \pi | \vec \alpha) \cdot P(\vec \mu | \vec \beta)\]

The generative process is as follows:
\begin{enumerate}
\item draw cluster proportions $\vec \pi \sim \text{Dir}(\vec \alpha)$
\item draw word proportions per cluster $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$
\item draw cluster assignments per document $\forall n \in N: \vec z_n \sim \text{Mult}(\vec \pi)$
\item draw word type per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the cluster selected by $\vec z_n$
\end{enumerate}

\end{Example}

\subsubsection{Other Probabilistic Modeling Languages}

Beside DGMs, with or without plate notation, there are other ways to express probabilistic models. The most exact way is to use \emph{mathematical formulas}, possibly combined with an algorithmic description. Graphical modeling languages provide a visual representation of the underlying model. Common examples are \emph{undirected graphical models} and \emph{factor graphs}, that focus on random variables and independence assumptions, as well as \emph{probabilistic relational models} and \emph{probabilistic entity-relationship models}, that try to combine relational data modeling and probabilistic modeling.

Mathematical formulas are a quick and effective way to describe a probabilistic process. They are the most exact representation and are ideally used together with a graphical language. A presentation solely based on formulas is not recommended because it might be hard to understand. In case of generative models, an algorithmic description of the generative process like in Example~\ref{ex:clustering-pm} aids the understanding. 

Similar to DGMs, undirected graphical models (\cite{kindermann1980markov}) represent variables as vertices, but the edges are undirected. Undirected graphical models, also known as Markov random fields, express the joint probability as a product of potential functions, normalized by the so called partition function. (\cite{bishop2006pattern},~p.~384-386) Reasoning about conditional independence is possible although the concept is slightly different than d-separation for DGMs. (\cite{bishop2006pattern},~p.~383-384)

Factor graphs have been developed by \textcite{kschischang2001factor} and extended by \textcite{frey2002extending} as a generalization of directed and undirected graphical models. A factor graph is a bipartite graph with two kinds of vertices: variables and factors. By modeling the factors as vertices, factor graphs are more explicit about the factorization of the joint probability. General inference algorithms like the sum-product algorithm (\cite{pearl1988probabilistic}) are easily expressed on factor graphs. However, the explicit notation brings verbosity and bigger models might become confusing.

Probabilistic relational models (PRMs) are a combination of the relational database model (\cite{codd1970relational}) and a probabilistic component. Developed by \textcite{friedman1999learning}, they have been extended by \textcite{getoor2003learning} to represent uncertainty of foreign keys. PRMs offer a nice way to express probabilistic relationships as an extension to a well-known notation in the database world.

Similar to the PRM that is an extension of the relational model, the probabilistic entity-relationship model (\cite{heckerman2007probabilistic},~p.~210~ff.) is an extension of ERMs (\cite{chen1976entity}). \cite{heckerman2007probabilistic} focus on directed acyclic probabilistic entity-relationship (DAPER) models, which enrich the standard ERM constructs by probabilistic relationships and dependencies between attributes. This notation is particularly interesting because it is close to ERMs, which are wide known to different professions and are on a higher abstraction level than the relational model.

\subsection{Inference on Probabilistic Models}
\label{subsec:inference}

Probabilistic inference refers to the task of estimating unknown quantities from known quantities (\cite{murphy2012machine},~p.~319). Although there is a difference between inference and learning in graphical model literature, we will use both terms interchangeably, as there is no distinction between inference and learning in a Bayesian view (\cite{murphy2012machine},~p.~320).

The following two subsections focus on inference algorithms for probabilistic models. The first part describes algorithms for arbitrary graphical models, while the second part discusses how custom implementations for different applications differ and what common implementations can do. Both, general and application specific implementations can be used to perform inference on models that are included in the data model driven framework.

\subsubsection{General Inference Algorithms for Graphical Models}

Depending on the structure of the graphical model and what is to be inferred, there are different algorithms at hand. Exact inference is only suitable for small problems. If the models grow bigger or contain cycles, exact inference can become intractable. This is where approximations jump in. We will focus on algorithms for directed and undirected graphical models and factor graphs. Inference for probabilistic relational models is discussed in \textcite{getoor2007introduction},~p.~159~ff.

In graphical models with a tree structure, one can use the \emph{sum-product} algorithm (\cite{pearl1988probabilistic}; \cite{kim1983computational}; \cite{pearl1982reverend}), also known as belief propagation, to exactly compute posterior marginals\footnote{In a Bayesian context, a posterior probability is the probability of hidden variables given the observations. Summing or integrating out all other variables than the one of interest is called \emph{marginalization}.}. It is efficient in the way that it shares computations in case that more than one marginal is required. By passing functions called messages between vertices, variables are marginalized out successively. This corresponds to a clever interchange of summations and products of the marginalized joint distribution whose factorization is given by the graph structure.

The \emph{max-sum} algorithm (\cite{bishop2006pattern},~p.~384-386; \cite{dawid1992applications}) can be used to infer a variable setting that has maximum probability. It is a derivate of the sum-product algorithm, where the sum is replaced by a max operator and the logarithm is taken to avoid numerical problems. Thus it can also be seen as a message passing algorithm.

The \emph{junction tree} algorithm (\cite{koller2009probabilistic},~p.~348~ff.; \cite{lauritzen1988local}) is an efficient way to perform exact inference on arbitrary graphs that may contain loops. It first triangulates the given graph and builds a junction tree from it. Then a two-stage message passing algorithm is applied to this junction tree. However, the computational complexity of the algorithm is determined by the size of the largest clique and grows exponentially.

In case exact inference is not tractable anymore, variation message passing algorithms like \emph{loopy belief propagation} (\cite{frey1998revolution}) can be used.

\subsubsection{Custom Inference Algorithm Implementations}
\label{subsec:custom-inference}

\begin{itemize}
\item Weka, Mallet, Mahout, Shark, Spark, ...
\item highly optimized inference algorithm implementations\\
$\Rightarrow$ instead of naive message passing algorithm like storage of all sums and stuff, the programmers know which stuff they want to use and need to store or which can be computed on the fly; it's not the update equations that differ but their implementation details
\end{itemize}

