\section{Probabilistic Models}

Probabilistic models are a powerful and flexible technique to model and reason about the real world. They are known to statisticians, machine learners and data miners alike. However, different users yield different modeling languages. The following subsections present common probabilistic modeling languages and how to use them to perform statistical inference.

\subsection{Probabilistic Modeling Languages}

Over the past decades, a variety of different probabilistic modeling languages have been used and developed. Graphical languages make it easy to understand the relationships between objects and which independence assumptions have been made. The focus in this work lies on a very popular language called \emph{directed graphical model} (DGM), or \emph{Bayesian network}, and an extension called \emph{plate model} that is going to be described in detail and used extensively throughout this thesis to represent the probabilistic view of the data. The last part of this subsection will give a quick overview of other alternatives.

\subsubsection{Directed Graphical Models} % \cite{bishop2006pattern},~p.~360~ff.;

DGMs are a widely-used graphical probabilistic modeling language (\cite{murphy2012machine},~p.~307~ff.; \cite{pearl1988probabilistic},~p.~116~ff.). They express variables and independence assumptions using a directed graph. Vertices represent variables, while an edge connecting two vertices expresses that both variables are dependent. Conversely, the absence of edges models independence. Hence a DGM induces a factorization of the joint probability, given by the graph structure. Each vertex $x$ forms a factor of the form $P\left(x|\text{pa}(x)\right)$, where the parents $\text{pa}(x)$ are all vertices connected with $x$ by incoming edges.

Figure~\ref{fig:simple_dgm} gives an example of a simple DGM with three variables. The final grade $g$ of an exam depends on the amount of class participation $c$ by that student and how much time $t$ he spent on preparing for the exam. The joint probability of this model factorizes into $$P(g,c,t) = P(g | c, t) \cdot P(c) \cdot P(t).$$ It is assumed that the two variables $c$ and $t$ are independent, given that $g$ is unknown. Reasoning about conditional independence can be done by using the framework of \emph{d-separation}  (\cite{lauritzen1996graphical},~p.~48~ff.; \cite{pearl1988probabilistic}).

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/simple_dgm.tex}}
	\end{center}
\caption[Simple directed graphical model containing three variables.]{Simple DGM containing three variables. The final grade of an exam is influenced by the student's class participation and how much time he spent on preparing for the exam.}
		\label{fig:simple_dgm}
\end{figure}

Following the notation of \textcite{bishop2006pattern}, vertices of observed variable are drawn as shaded circles, while hyper-parameters\footnote{Hyper-parameters are a concept of Bayesian statistics. In a Bayesian world, the parameter of a probability distribution is also a random variable. It is distributed according to a prior distribution having another set of parameters, which are called hyper-parameters.} are drawn as small, black circles. This helps to distinguish them from normal parameters and hidden variables.\footnote{Formally, there is no difference between parameters and hidden variables. Both are vertices in the DGM and represent random variables, but while the number of parameters remains constant, the number of hidden variables might increase with an increasing amount of data.}

\subsubsection{Plate Models}

DGMs are natural for modeling flat data, i.e.~single variables and their dependencies. When dealing with hierarchical data, traditional DGMs become verbose and hard to read. Consider a set of $n$ students, all writing the same exam. This would require to repeat the vertices from Figure~\ref{fig:simple_dgm} $n$ times to capture this repeated measurement. If one wants to model multiple exams, professors or even universities, the resulting model becomes inscrutable.

Plate models (\cite{buntine1994operations}; \cite{gilks1994language}) introduce a mechanism for a compact notation of those repeated measurements. Instead of explicitly repeating a variable $n$ times, it is covered by a plate that has the cardinality $n$, and an index is added to the variable, e.g.\ $g_s$ is the grade of student $s$. The index set is usually written in one of the plate's corners, e.g.\ all students are denoted by $S$. If we provide a telling name for the index set, we add the set variable in brackets, e.g.\ Students ($S$). We follow the widely-used convention that the index variable is denoted by the small letter corresponding to the capitalized index set variable, if not stated otherwise.

To extend the students example we model a grade $g_s$, an amount of  participation $c_s$, and a preparation time $t_s$ for every student $s \in S$ as shown in Figure~\ref{fig:simple_platemodel}. We also add an additional variable $a$ outside of the Students plate, measuring the average grade of all students. The edge between the individual grade and the average one leaves the student plate, which reflects the natural dependency between an average and the individual observations.

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/simple_platemodel.tex}}
	\end{center}
\caption[Simple plate model containing three variables.]{Simple plate model containing three variables. The model contains a plate to capture the repeated measurement of the variables for every student. Each student $s \in S$ has his own amount of class participation $c_s$, exam preparation time $t_s$ and final grade $g_s$. Additionally, the average of all grades is measured as another variable $a$ outside of the plate.}
		\label{fig:simple_platemodel}
\end{figure}

\textcite{heckerman2007probabilistic} state that traditional plate models require edge constraints to be more expressive. Those constraints are often implicit: In the expanded graph of Figure~\ref{fig:simple_platemodel}, there are only edges between variables inside the Students plate if they have the same index $s$. Each student is assumed statistically independent of all the others. See Figure~\ref{fig:appendix_hmm}, Appendix C for a model with explicit edge constraints.

\begin{Example} Plate model for Dirichlet multinomial text clustering
\label{ex:clustering-pm}

A typical way to model text data is to model documents as a list of independent tokens, in a so called \emph{bag-of-words} model (\cite{murphy2012machine},~pp.~87-89). It has been found that modeling the word counts per document using a multinomial distribution yields better classification performance than using a Bernoulli model (\cite{mccallum1998comparison}). Depending on if there is a supervised or unsupervised scenario, this model can be used for classification or clustering.\footnote{When the cluster assignments $\vec z_n$ are known (supervised learning), exact inference can be performed. The parameter estimations can be used to classify new documents. If the cluster assignments are unknown (unsupervised learning), they also have to be inferred using approximate inference. More on inference in probabilistic models can be found in Section~\ref{subsec:inference}.} For this thesis we shall use a Bayesian approach for clustering documents using the bag-of-words model as a universal example. As it uses Dirichlet distributed priors for the multinomial distribution parameters, it is called \emph{Dirichlet multinomial clustering}.

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/clustering_transformation_platemodel.tex}}
	\end{center}
\caption[Plate model for Dirichlet multinomial clustering of text documents]{Plate model for Dirichlet multinomial clustering of text documents. A document $n$ consists of a set of tokens $M_n$. Each token is of exactly one word type, denoted by $\vec d_{nm}$. The cluster assignment per document is encoded by $z_n$. The cluster proportions are represented by $\vec \pi$, while each cluster $k$ has its own word distribution parameterized by $\vec \mu_k$.}
		\label{fig:clustering_platemodel}
\end{figure}

Let $N$ be a collection of documents, where each document $n \in N$ consists of a set of tokens $M_n$. A token $m \in M_n$ is of exactly one token type, called \emph{word}, that comes from a vocabulary $V$. The vocabulary contains all token types of the collection. The type of each token is denoted by $\vec d_{nm}$, which is a $|V|$-dimensional 1-of-K coded variable (\cite{bishop2006pattern},~p.~424) having exactly a single 1 at the index associated with $v$ if the token $m$ is of word type $v$. Each document is assigned to a cluster $k \in K$. This assignment is 1-of-K coded in $\vec z_{n}$, which has a 1 at the index associated with $k$ if the document $n$ is assigned to cluster $k$. Each cluster has its own word distribution parameterized by $\vec \mu_k$, while the global cluster proportions are represented as $\vec \pi$. To adopt a Bayesian view, we add hyper-parameters $\vec \alpha$ and $\vec \beta_k$ for $\vec \pi$ and $\vec \mu_k$, respectively. The plate model corresponding to this example problem is shown in Figure~\ref{fig:clustering_platemodel}.

The variables have the following constraints:
\begin{align}
\sum_{k \in K} z_{nk} &= 1,\quad \text{a document is assigned to exactly one cluster}\\
\sum_{v \in V} d_{nmv} &= 1,\quad \text{a token is of exactly one word type}\\
\sum_{k \in K} \pi_{k} &= 1,\quad \text{the cluster proportions sum to one}\\
\sum_{v \in V} \mu_{kv} &= 1,\quad \text{the word distribution per cluster sums to one}
\end{align}

The joint distribution factorization is given as
\begin{multline}
P(\boldsymbol{d},\boldsymbol{z}, \boldsymbol{\mu}, \vec \pi | \vec \alpha, \boldsymbol{\beta}) = \\
 P(\vec \pi | \vec \alpha) \cdot
 \left(\prod_{n \in N} P(\vec z_n | \vec \pi)\right) \cdot
 \left(\prod_{k \in K}P(\vec \mu_k | \vec \beta_k)\right) \cdot 
 \left(\prod_{n \in N} \prod_{m \in M_n} \prod_{k \in K}P(\vec d_{nm} | \vec z_n, \vec \mu_k)\right),
\end{multline}
where $\boldsymbol{d} = \{\vec d_{nm} : n \in N \land m \in M_n \}$ is the set comprising all token vectors, $\boldsymbol{z} = \{ \vec z_n : n \in N \}$ are the cluster assignments of all documents, $\boldsymbol{\mu} = \{ \vec \mu_k : k \in K \}$ represents the word proportions of all clusters and $\boldsymbol{\beta} = \{ \vec \beta_k : k \in K \}$ denotes the set of all hyper-parameters for the word proportions.

The generative process is as follows:
\begin{enumerate}
\item Draw cluster proportions $\vec \pi \sim \text{Dir}(\vec \alpha)$.
\item Draw word proportions per cluster $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$.
\item Draw cluster assignments per document $\forall n \in N: \vec z_n \sim \text{Mult}(\vec \pi)$.
\item Draw word type per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the cluster selected by $\vec z_n$.
\end{enumerate}

\end{Example}

\subsubsection{Other Probabilistic Modeling Languages}

Beside DGMs -- with or without plate notation -- there are other ways to express probabilistic models. The most exact way is to use \emph{mathematical formulas}, best combined with a textual description. Graphical modeling languages provide a visual representation of the underlying model. Common examples are DGMs, \emph{undirected graphical models}, and \emph{factor graphs}, that focus on random variables and independence assumptions, as well as \emph{probabilistic relational models} and \emph{probabilistic entity-relationship models}, that try to combine relational data modeling and probabilistic modeling. The following paragraphs introduce each of the methods mentioned above in a very short manner.

Mathematical formulas are a brief and effective way to describe a probabilistic process. They are the most exact representation and are ideally used together with a graphical language. A presentation solely based on formulas is not recommended because it might be intimidating, especially for users that do not have a strong mathematical background. In case of generative models, an algorithmic description of the generative process like in Example~\ref{ex:clustering-pm} aids the understanding. 

Similar to DGMs, undirected graphical models (\cite{kindermann1980markov}) represent variables as vertices, but the edges are undirected. Undirected graphical models, also known as Markov random fields, express the joint probability as a product of so called potential functions, normalized by a partition function (\cite{bishop2006pattern},~pp.~384-386). Reasoning about conditional independence is possible, although the concept is slightly different than d-separation for DGMs (\cite{bishop2006pattern},~pp.~383-384).

Factor graphs have been developed by \textcite{kschischang2001factor} and extended by \textcite{frey2002extending} as a generalization of directed and undirected graphical models. A factor graph is a bipartite graph with two kinds of vertices: variables and factors. By modeling the factors as vertices, factor graphs are more explicit about the factorization of the joint probability. General inference algorithms like the sum-product algorithm (\cite{pearl1988probabilistic}) are easily expressed on factor graphs. However, the explicit notation brings verbosity and larger models might become confusing.

Probabilistic relational models (PRMs) are a combination of the relational database model (\cite{codd1970relational}) and a probabilistic component. Developed by \textcite{friedman1999learning}, they have been advanced by \textcite{getoor2003learning} to represent uncertainty of foreign keys. PRMs offer a way to express probabilistic relationships as an extension to a well-known notation in the database world.

Like PRMs are an extension of the relational model, probabilistic entity-relationship models (\cite{heckerman2007probabilistic},~p.~210~ff.) are an extension of ERMs (\cite{chen1976entity}). The authors focus on directed acyclic probabilistic entity-relationship (DAPER) models, which enrich the standard ERM constructs by probabilistic relationships and dependencies between attributes. This notation is particularly interesting because it is close to ERMs, which are widely known among different professions and are on a higher abstraction level than the relational model.

\subsection{Inference in Graphical Models}
\label{subsec:inference}

Probabilistic inference refers to the task of estimating unknown quantities from known ones (\cite{murphy2012machine},~p.~319). Although a difference is being made between inference and learning in graphical model literature, we will use both terms interchangeably, as there is no distinction between inference and learning in a Bayesian view (\cite{murphy2012machine},~p.~320).

The following paragraphs focus on inference algorithms for probabilistic graphical models. First, we describe algorithms for arbitrary graphical models, followed by a discussion about the difference between generic inference algorithm implementations and model specific ones. Both, generic and model specific implementations can be used to perform inference on models that are included in the data model driven framework.

\subsubsection{Inference Algorithms for Graphical Models}

Depending on the structure of the graphical model and what is to be inferred, there are different algorithms available. Exact inference is only suitable for small problems. If the models grow bigger or contain cycles, exact inference quickly becomes intractable. Approximate algorithms help to cope with this problem. We will focus on algorithms for directed and undirected graphical models as well as factor graphs. Inference for probabilistic relational models is discussed in \textcite{getoor2007introduction},~p.~159~ff.

In graphical models with a tree structure, one can use the \emph{sum-product} algorithm (\cite{pearl1988probabilistic}; \cite{kim1983computational}), also known as belief propagation, to exactly compute posterior marginals.\footnote{In a Bayesian context, a posterior probability is the probability of hidden variables given the observations. Summing or integrating out all other variables than the ones of interest is called \emph{marginalization}.} Variables are marginalized out successively by passing functions between vertices. These functions are called messages, hence belief propagation is a message passing algorithm. It corresponds to a clever interchange of summations and products of the marginalized joint distribution whose factorization is given by the graph structure. 

The \emph{max-sum} algorithm (\cite{dawid1992applications}) can be used to infer a variable setting that has maximum probability. It is a derivate of the sum-product algorithm, where the sum is replaced by a max operator and the logarithm of the joint distribution is taken to avoid numerical problems. It can also be viewed as a message passing algorithm.

The \emph{junction tree} algorithm (\cite{lauritzen1988local}) is an efficient way to perform exact inference on arbitrary graphs that may contain loops. It first triangulates\footnote{A triangulated (chordal) graph induces only loops that are triangles, i.e.~contain at most three vertices.  See \textcite{fulkerson1965incidence} or \textcite{rose1976algorithmic} for more details.} the given graph and builds a junction tree from it, whose nodes correspond to the maximal cliques of the triangulated graph. Then a two-stage message passing algorithm is applied to this junction tree. However, the computational complexity of the algorithm is determined by the size of the largest clique and grows exponentially.

In case exact inference is not tractable anymore, approximations like \emph{loopy belief propagation} (\cite{frey1998revolution}) can be used. The idea is to apply the sum-product algorithm that is constructed for trees on the graph containing loops. However, it is model dependent whether passing the messages in circles will converge, and not guaranteed to yield good results.

Another approximation technique is \emph{variational message passing} (\cite{winn2005variational}). In contrast to belief propagation, it uses factorized variational approximation. This corresponds to additional independence assumptions for the posterior distribution which results in an approximation of the true posterior. There is no general rule which independence assumptions to make but the quality of the results depends on the chosen factorization.

With all these inference algorithms available, it is possible to perform inference on almost any given graphical model. When including a new model together with an algorithm into the data model driven framework, the machine learning developer needs to decide whether he chooses a generic implementation of one of the algorithms described above, or provides a custom one. The next paragraphs explain ways of implementing probabilistic inference algorithms.

\subsubsection{Inference Algorithm Implementations}
\label{subsec:custom-inference}

Implementing the algorithms described above, one can perform inference on arbitrary graphical models. To allow the implementations to be efficient, it is often required that the probability distributions used in the models are from the exponential family and conjugate priors are chosen.\footnote{For more information on the exponential family and why it is important see \textcite{murphy2012machine},~p.~348~ff. and \textcite{andersen1970sufficiency}.} Infer.NET (\cite{InferNET12}) offers message passing algorithms that can be applied on user defined models. BUGS (\cite{lunn2009bugs}) focuses on the inference in probabilistic models using Gibbs sampling (\cite{geman1984stochastic}). Both libraries allow the user to specify their own models using a special language. Although this approach is very flexible, it is slower than optimized custom implementations for the given model.

Custom implementations can extensively optimize the way the update equations or messages are implemented. They can choose efficient data structures and implicitly code information, saving memory and computation time. Due to this fact, a good custom implementation of an inference algorithm is faster than, e.g.\ using Infer.NET and specifying the exact same model there.

There is a great diversity of software available that offer model specific inference algorithm implementations. We only name a few to give the reader an impression. MALLET (\cite{mccallum2002mallet}) offers implementations for clustering and topic models as well as classification tasks. Weka (\cite{hall2009weka}) includes a wide variety of models like Bayesian logistic regression (\cite{genkin2007large}), discriminate multinomial naive Bayes classifiers (\cite{talia2005weka4ws}) and many more. Apache Mahout (\cite{mahout}) provides distributed algorithms for a broad set of models.

We suggest to use a custom implementation for models included in the data model driven framework. However, it could be useful to include generic implementations into the framework that will be used if no custom implementation is provided.

To select a probabilistic model combined with an inference algorithm implementation, an analyst can browse through different models and choose the one that fits the data. In the case of the data model driven approach, available probabilistic models are presented as ERMs. The next section goes into detail about translating plate models to ERMs and how to integrate them into the domain specific data model.
