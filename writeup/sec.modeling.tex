\section{Probabilistic Models}

Probabilistic models are a mighty and flexible technique to model and reason about the real world. They are known to statisticians, machine learners and data miners alike. However, different users yield different modeling languages. The following sections present common probabilistic modeling languages and how to use them to perform statistical inference.

\subsection{Probabilistic Modeling Languages}

Over the past decades, a variety of different probabilistic modeling languages have been used and developed. Despite the most general one, using mathematical formulas only, we want to highlight graphical representations that often make it easier to understand the relationships between objects and which independence assumptions have been made. We focus on a very popular one called \emph{directed graphical model} (DGM), or \emph{Bayesian network}, and an extension called \emph{plate model} that is going to be described in detail and used extensively throughout this thesis to represent the probabilistic view of the data. The last part of this subsection will give a quick overview of other alternatives.

\subsubsection{Directed Graphical Models}

DGMs are a well known and powerful graphical probabilistic modeling language (\cite{murphy2012machine}~p.~307~ff.; \cite{bishop2006pattern},~p.~360~ff.; \cite{pearl1988probabilistic},~p.~116~ff.). They express variables and independence assumptions using a directed graph. Vertices represent variables, while the absence of edges models that the two variables not connected are assumed to be independent. That means that a DGM induces a factorization of the joint probability, given by the graph structure. [TODO: Explain this factorization in greater detail (vertices depend on parents)]

Figure~\ref{fig:simple_dgm} shows a simple DGM with three variables. The final grade $g$ of an exam depends on the amount of class participation $c$ by that student and how much time $t$ he spent to prepare for the exam. The joint probability of this model factorizes into $$P(g,c,t) = P(g | c, t) \cdot P(c) \cdot P(t).$$ It is assumed that the two variables $c$ and $t$ are independent, given that $g$ is unknown. Reasoning about conditional independence can be done by using the framework of \emph{d-separation}  (\cite{lauritzen1996graphical},~p.~48~ff.; \cite{pearl1988probabilistic}).

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/simple_dgm.tex}}
	\end{center}
\caption[Simple directed graphical model containing three variables.]{Simple DGM containing three variables. The final grade of an exam is influenced by the students class participation and how much time he spent to prepare for the exam.}
		\label{fig:simple_dgm}
\end{figure}

Following the notation of \textcite{bishop2006pattern}, vertices of observed variable are drawn as shaded circles, while hyper-parameters\footnote{Hyper-parameters are a concept of Bayesian statistics. In a Bayesian world, the parameter of a probability distribution, for example the probability to see heads when tossing a coin, is also a random variable. It is sampled from a prior distribution having another set of parameters. Those are called hyper-parameters. Prior distributions are used to encode prior knowledge into the model and to avoid the zero count problem, similar to Laplace smoothing (\cite{manning2008introduction},~p.~260).} are drawn as small, black circles. This helps to distinguish them from normal parameters and hidden variables.\footnote{Formally there is no difference between parameters and hidden variables. Both are vertices in the DGM and represent random variables, but while the number of parameters remains constant, the number of hidden variables might increase with an increasing amount of data.}

\subsubsection{Plate Models}

DGMs are natural for modeling flat data, i.e. single variables and their dependencies. When it comes to hierarchical data, traditional DGMs become verbose and hard to read. Consider a set of $s$ students, all writing the same exam. This would require to repeat the vertices from Figure~\ref{fig:simple_dgm} $s$ times to capture this repeated measurement. If one wants to model multiple exams, professors or even universities, the resulting model becomes inscrutable.

Plate models (\cite{buntine1994operations}; \cite{gilks1994language}) introduce a mechanism for a compact notation of those repeated measurements. Instead of explicitly repeating a variable $s$ times, it is covered by a plate that has the cardinality $s$ and an index is added to the variable. So for every student $s \in S$ there is a final grade $g_s$, an amount of class participation $c_s$ and an exam preparation time $t_s$. We also added an additional variable outside of the students plate, measuring the average grade of all students. As the edge from the individual grade to this aggregated variable leaves the student plate, it reflects the dependency of the average grade on the grades from all students.

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/simple_platemodel.tex}}
	\end{center}
\caption[Simple plate model containing three variables.]{Simple plate model containing three variables. The model now contains a plate to capture the repeated measurement of the variables for every student of the class. Additionally, the average of all grades is measured as another variable outside of the plate.}
		\label{fig:simple_platemodel}
\end{figure}

\textcite{heckerman2007probabilistic} state that traditional plate models require edge constraints to be more expressive. Those constraints are often implicit: In the expanded graph of Figure~\ref{fig:simple_platemodel}, there are only edges between variables inside the Students plate if they belong to the same student (have the same index $s$). Each student is statistically independent of all the others.

\begin{Example} Plate model for Dirichlet multinomial text clustering
\label{ex:clustering-pm}

A typical way to model text data is to model documents as a list of independent tokens, in a so called \emph{bag-of-words} model (\cite{murphy2012machine},~p.~87-89). It has been found that modeling the word counts per document using a multinomial distribution yields better classification performance than using a Bernoulli model (\cite{mccallum1998comparison}). Depending on if there is a supervised or unsupervised scenario, this model can be used for classification or clustering, respectively.\footnote{When the cluster assignments $\vec z_n$ are known (supervised learning), the inference of the parameters is exact. The parameter estimations can be used to classify new documents. If the cluster assignments are unknown (unsupervised learning), they also have to be inferred using approximate inference. More on inference in probabilistic models can be found in Section~\ref{subsec:inference}.} For this thesis we shall use a Bayesian approach of this bag-of-words model for clustering documents as a universal example. As it uses Dirichlet distributed priors for $\vec \mu_k$ and $\vec \pi$, it is called \emph{Dirichlet multinomial clustering}.

\begin{figure}[t]
	\begin{center}
    	\scalebox{\tikzScale}{\adjustTikzSize \input{img/clustering_transformation_platemodel.tex}}
	\end{center}
\caption[Plate model for Dirichlet multinomial clustering of text documents]{Plate model for Dirichlet multinomial clustering of text documents.}
		\label{fig:clustering_platemodel}
\end{figure}

Given a collection of documents $N$, where each document $n \in N$ consists of a set of tokens $M_n$. A token $m$ is of exactly one token type, called \emph{word}, that comes from a vocabulary $V$. The type of each token is denoted by $\vec d_{nm}$, which is a $|V|$ dimensional 1-of-K coded variable (\cite{bishop2006pattern},~p.~424) having exactly a single 1 at position $v$ if the token $m$ is of word type $v$. Each document is assigned to a cluster $k \in K$, 1-of-K coded in $\vec z_{n}$, which has a 1 at position $k$ if the document $n$ is assigned to cluster $k$. Each cluster has its own word distribution parameterized by $\vec \mu_k$. The plate model is shown in Figure~\ref{fig:clustering_platemodel}.

The variables have the following constraints:
\begin{align}
\sum_{k \in K} z_{nk} &= 1,\quad \text{a document is assigned to exactly one cluster}\\
\sum_{v \in V} d_{nmv} &= 1,\quad \text{a token is of exactly one word type}\\
\sum_{k \in K} \pi_{k} &= 1,\quad \text{the cluster proportions sum to one}\\
\sum_{v \in V} \mu_{kv} &= 1,\quad \text{the word distribution per cluster sums to one}
\end{align}

The joint distribution factorization is given as
\begin{multline}
P(\boldsymbol{d},\boldsymbol{z}, \boldsymbol{\mu}, \vec \pi | \vec \alpha, \boldsymbol{\beta}) = \\
 P(\vec \pi | \vec \alpha) \cdot
 \left(\prod_{n \in N} P(\vec z_n | \vec \pi)\right) \cdot
 \left(\prod_{k \in K}P(\vec \mu_k | \vec \beta_k)\right) \cdot 
 \left(\prod_{n \in N} \prod_{m \in M_n} \prod_{k \in K}P(\vec d_{nm} | \vec z_n, \vec \mu_k)\right).
\end{multline}

The generative process is as follows:
\begin{enumerate}
\item draw cluster proportions $\vec \pi \sim \text{Dir}(\vec \alpha)$
\item draw word proportions per cluster $\forall k \in K: \vec \mu_k \sim \text{Dir}(\vec \beta_k)$
\item draw cluster assignments per document $\forall n \in N: \vec z_n \sim \text{Mult}(\vec \pi)$
\item draw word type per token $\forall n \in N, m \in M_n: \vec d_{nm} \sim \text{Mult}(\vec \mu_k)$, where $k$ is the cluster selected by $\vec z_n$
\end{enumerate}

\end{Example}

\subsubsection{Other Probabilistic Modeling Languages}

Beside DGMs, with or without plate notation, there are other ways to express probabilistic models. The most exact way is to use \emph{mathematical formulas}, best combined with a textual description. Graphical modeling languages provide a visual representation of the underlying model. Common examples are DGMs, \emph{undirected graphical models} and \emph{factor graphs}, that focus on random variables and independence assumptions, as well as \emph{probabilistic relational models} and \emph{probabilistic entity-relationship models}, that try to combine relational data modeling and probabilistic modeling. The following paragraphs introduce each of the methods mentioned above in a very short manner.

Mathematical formulas are a quick and effective way to describe a probabilistic process. They are the most exact representation and are ideally used together with a graphical language. A presentation solely based on formulas is not recommended because it might be hard to understand, especially for users that do not have a strong mathematical background. In case of generative models, an algorithmic description of the generative process like in Example~\ref{ex:clustering-pm} aids the understanding. 

Similar to DGMs, undirected graphical models (\cite{kindermann1980markov}) represent variables as vertices, but the edges are undirected. Undirected graphical models, also known as Markov random fields, express the joint probability as a product of potential functions, normalized by the so called partition function. (\cite{bishop2006pattern},~p.~384-386) Reasoning about conditional independence is possible although the concept is slightly different than d-separation for DGMs. (\cite{bishop2006pattern},~p.~383-384)

Factor graphs have been developed by \textcite{kschischang2001factor} and extended by \textcite{frey2002extending} as a generalization of directed and undirected graphical models. A factor graph is a bipartite graph with two kinds of vertices: variables and factors. By modeling the factors as vertices, factor graphs are more explicit about the factorization of the joint probability. General inference algorithms like the sum-product algorithm (\cite{pearl1988probabilistic}) are easily expressed on factor graphs. However, the explicit notation brings verbosity and bigger models might become confusing.

Probabilistic relational models (PRMs) are a combination of the relational database model (\cite{codd1970relational}) and a probabilistic component. Developed by \textcite{friedman1999learning}, they have been extended [TODO: remove 'extended' repetition] by \textcite{getoor2003learning} to represent uncertainty of foreign keys. PRMs offer a nice way to express probabilistic relationships as an extension to a well-known notation in the database world.

Similar to the PRM that is an extension of the relational model, the probabilistic entity-relationship model (\cite{heckerman2007probabilistic},~p.~210~ff.) is an extension of ERMs (\cite{chen1976entity}). \cite{heckerman2007probabilistic} focus on directed acyclic probabilistic entity-relationship (DAPER) models, which enrich the standard ERM constructs by probabilistic relationships and dependencies between attributes. This notation is particularly interesting because it is close to ERMs, which are wide known to different professions and are on a higher abstraction level than the relational model.

\subsection{Inference in Graphical Models}
\label{subsec:inference}

Probabilistic inference refers to the task of estimating unknown from known quantities (\cite{murphy2012machine},~p.~319). Although there is a difference between inference and learning in graphical model literature, we will use both terms interchangeably, as there is no distinction between inference and learning in a Bayesian view (\cite{murphy2012machine},~p.~320).

The following paragraphs focus on inference algorithms for probabilistic graphical models. First, we describe algorithms for arbitrary graphical models, followed by a discussion about the difference between generic inference algorithm implementations and model specific ones. Both, generic and model specific implementations can be used to perform inference on models that are included in the data model driven framework.

\subsubsection{Inference Algorithms for Graphical Models}

Depending on the structure of the graphical model and what is to be inferred, there are different algorithms at hand. Exact inference is only suitable for small problems. If the models grow bigger or contain cycles, exact inference quickly becomes intractable. This is where approximations jump in. We will focus on algorithms for directed and undirected graphical models as well as factor graphs. Inference for probabilistic relational models is discussed in \textcite{getoor2007introduction},~p.~159~ff.

In graphical models with a tree structure, one can use the \emph{sum-product} algorithm (\cite{pearl1988probabilistic}; \cite{kim1983computational}; \cite{pearl1982reverend}), also known as belief propagation, to exactly compute posterior marginals\footnote{In a Bayesian context, a posterior probability is the probability of hidden variables given the observations. Summing or integrating out all other variables than the one of interest is called \emph{marginalization}.}. It shares computations in case that more than one marginal is required. By passing functions called messages between vertices, variables are marginalized out successively. This corresponds to a clever interchange of summations and products of the marginalized joint distribution whose factorization is given by the graph structure.

The \emph{max-sum} algorithm (\cite{bishop2006pattern},~p.~384-386; \cite{dawid1992applications}) can be used to infer a variable setting that has maximum probability. It is a derivate of the sum-product algorithm, where the sum is replaced by a max operator and the logarithm is taken to avoid numerical problems. It can also be viewed as a message passing algorithm.

The \emph{junction tree} algorithm (\cite{koller2009probabilistic},~p.~348~ff.; \cite{lauritzen1988local}) is an efficient way to perform exact inference on arbitrary graphs that may contain loops. It first triangulates\footnote{A triangulated (chordal) graph induces only loops that are triangles, i.e. contain at most three vertices.  See \textcite{fulkerson1965incidence} or \textcite{rose1976algorithmic} for more details.} the given graph and builds a junction tree from it, whose nodes correspond to the maximal cliques of the triangulated graph. Then a two-stage message passing algorithm is applied to this junction tree. However, the computational complexity of the algorithm is determined by the size of the largest clique and grows exponentially.

In case exact inference is not tractable anymore, approximations like \emph{loopy belief propagation} (\cite{frey1998revolution}) can be used. The idea is to apply the sum-product algorithm that is constructed for trees on the graph containing loops. However, it is model dependent if passing the messages in circles will converge and not guaranteed to yield good results.

Another approximation technique is variational message passing (\cite{winn2005variational}) which is also a message passing algorithm. The difference to belief propagation, it uses factorized variational approximation. This corresponds to additional independence assumptions for the posterior distribution which results in an approximation of the true posterior. There is no general rule which independence assumptions to make but the quality of the results depends on the chosen factorization.

\subsubsection{Inference Algorithm Implementations}
\label{subsec:custom-inference}

Implementing the algorithms described above, one can perform inference on arbitrary graphical models. To allow the implementations to be efficient, it is often required that the probability distributions used in the models are from the exponential family and conjugate priors are used.\footnote{For more information on the exponential family and why it is important see \textcite{murphy2012machine},~p.~348~ff. and \textcite{andersen1970sufficiency}.} Infer.NET (\cite{InferNET12}) offers message passing algorithms that can be applied on user defined models. BUGS (\cite{lunn2009bugs}) focuses on the inference in probabilistic models using Gibbs sampling (\cite{geman1984stochastic}). Both libraries allow the user to specify their own models using a special language. Although this approach is very flexible, it is often slower than custom implementations for the given model.

Custom implementations can extensively optimize the way the update equations or messages are implemented, choose efficient data structures and implicitly code information, saving memory and computation time. Due to this fact it might be faster to use a custom implementation of a special algorithm than using Infer.NET and specifying the exact same model there.

There is a great diversity of software available that offer model specific inference algorithm implementations. We only want to name a few to give the reader an impression. MALLET (\cite{mccallum2002mallet}) offers implementations for clustering and topic models as well as classification tasks. WEKA (\cite{hall2009weka}) includes a wide variety of models like Bayesian logistic regression (\cite{genkin2007large}), discriminate multinomial naive Bayes classifiers (\cite{talia2005weka4ws}) and many more. Apache Mahout (\cite{mahout}) provides distributed algorithms for a broad set of models.

We suggest to use a custom implementation for models included in the data model driven framework. However, it could be useful to include generic implementations into the framework that will be used if no custom implementation is provided.
